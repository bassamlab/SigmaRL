import os
import sys
import time

# !Important: Add project root to system path if you want to run this file directly
script_dir = os.path.dirname(__file__) # Directory of the current script
project_root = os.path.dirname(script_dir) # Project root directory
if project_root not in sys.path:
    sys.path.append(project_root)
    
import torch

from vmas import render_interactively
from vmas.simulator.core import Agent, Box, World, Line
# from vmas.simulator.dynamics.kinematic_bicycle import KinematicBicycle
from vmas.simulator.scenario import BaseScenario
from vmas.simulator.utils import Color

from torch import Tensor
from typing import Dict

import numpy as np


import matplotlib.pyplot as plt
from matplotlib.patches import Rectangle

from utilities.helper_scenario import Collisions, Distances, Evaluation, Normalizers, Observations, Penalties, ReferencePathsMapRelated, Rewards, Thresholds, Obstacles, Timer, exponential_decreasing_fcn, get_distances_between_agents, get_perpendicular_distances, get_rectangle_corners, get_ref_path_for_obstacle_avoidance_scenarios, get_short_term_reference_path, interX, get_point_line_distance, normalize_angle, transform_from_global_to_local_coordinate

from utilities.helper_training import Parameters

from utilities.kinematic_bicycle import KinematicBicycle

# Get road data
from utilities.get_cpm_lab_map import get_map_data, get_center_length_yaw_polyline

from utilities.colors import Color

# Sample time
dt = 0.1 # [s]

# Geometry
agent_width = 0.10              # The width of the agent in [m]
agent_length = 0.20             # The length of the agent in [m]
agent_wheelbase_front = 0.10    # Front wheelbase in [m]
agent_wheelbase_rear = agent_length - agent_wheelbase_front # Rear wheelbase in [m]

# Maximum control commands
agent_max_speed = 0.5          # Maximum speed in [m/s]
agent_max_steering_angle = 35   # Maximum steering angle in degree

n_agents = 1        # The number of agents
agent_mass = 1    # The mass of each agent in [m]

# Reward
reward_progress = 2
reward_speed = 0.2 # ! Should not be a dominant reward, which may encourage agents to move in a high speed but not the right path or direction
reward_reach_goal = 50 # Reward for reaching goal
reward_reach_intermediate_goals = 2 # Reward for reaching intermediate goals

# Penalty for deviating from reference path
penalty_deviate_from_ref_path = -0.1
penalty_deviate_from_goal = -1
# Penalty for leaving the world boundary
penalty_leave_world = -25
# Penalty for changing steering direction
penalty_change_steering = -0.0
# Penalty for losing time
penalty_time = -0.0
# Penalty for colliding with obstacles
penalty_collide_with_obstacles = -25

# Reference path
n_points_short_term = 12 # The number of points on the short-term reference path

# Observation
is_global_coordinate_sys = True # Set to True if you want to use global coordinate system
is_local_observation = False # TODO Set to True if sensor range is limited
is_add_noise = False
noise_level = 0.2 * agent_width # Noise will be generated by the standary normal distribution. This parameter controls the noise level
n_stored_steps = 5 # The number of steps to store (include the current step). At least one.
n_observed_steps = 1 # The number of steps to observe (include the current step). At least one. At most `n_stored_steps`
n_nearing_obstacles_observed = 4 # The number of nearing obstacles to be observed (consider limited sensor range)
is_observe_corners = False
is_use_intermediate_goals = True

is_testing_mode = False # In testing mode, collisions do not lead to the termination of the simulation 
is_visualize_short_term_path = True

threshold_reach_goal = agent_width / 2 # Agents are considered at their goal positions if their distances to the goal positions are less than this threshold
threshold_reach_intermediate_goal = agent_width / 2 # Agents are considered at their intermediate goal positions if their distances to the goal positions are less than this threshold
threshold_change_steering = 10 # [degree]

max_steps = 2000
is_dynamic_goal_reward = True

# Current implementation includes two obstacle-avoidance types
obstacle_avoidance_types = [
    "static",         # Tracking a straight vertical line with the initial velocity having an initial angle, e.g., 45 degrees
    "dynamic",      # Tracking a turning path (90 degrees)
]
path_tracking_type_default = obstacle_avoidance_types[0]

def generate_random_convex_polygon(n: int = 10, x_range: float = 1.0, y_range: float = 1.0, center_point = None, device = torch.device("cpu")) -> (torch.Tensor):
    """
    Inspired by https://cglab.ca/~sander/misc/ConvexGeneration/convex.html#valtr
    
    Generates a random convex polygon with n vertices.

    The function works by first generating two sorted lists of random x and y coordinates.
    These coordinates are then used to create vectors that are sorted by angle.
    The sorted vectors are laid end-to-end to form the convex polygon.
    Finally, the polygon is shifted to match the original minimum x and y coordinates.

    Parameters:
    n (int): The number of vertices for the convex polygon.
    x_range: x-coordinates range from 0 to `x_range`
    y_range: y-coordinates range from 0 to `y_range`
    center_point: torch.Size([2]), the center of the polygon will be shift to this point

    Returns:
    points_tensor: size [n + 1, 2], represents the vertices of the convex polygon (the first point is repeated at the end to close the polygon).
    """

    # Generate two lists of random X and Y coordinates using PyTorch
    x_pool = torch.rand(n, device=device, dtype=torch.float32) * x_range
    y_pool = torch.rand(n, device=device, dtype=torch.float32) * y_range

    # Sort them
    x_pool, _ = torch.sort(x_pool)
    y_pool, _ = torch.sort(y_pool)

    # Isolate the extreme points
    min_x, max_x = x_pool[0], x_pool[-1]
    min_y, max_y = y_pool[0], y_pool[-1]

    # Divide the interior points into two chains & Extract the vector components
    x_vec, y_vec = [], []
    last_top, last_bot = min_x, min_x
    last_left, last_right = min_y, min_y

    for x in x_pool[1:-1]:
        if torch.rand(1).item() > 0.5:
            x_vec.append(x - last_top)
            last_top = x
        else:
            x_vec.append(last_bot - x)
            last_bot = x

    x_vec.extend([max_x - last_top, last_bot - max_x])

    for y in y_pool[1:-1]:
        if torch.rand(1).item() > 0.5:
            y_vec.append(y - last_left)
            last_left = y
        else:
            y_vec.append(last_right - y)
            last_right = y

    y_vec.extend([max_y - last_left, last_right - max_y])

    # Convert lists to tensors
    x_vec = torch.tensor(x_vec, device=device, dtype=torch.float32)
    y_vec = torch.tensor(y_vec, device=device, dtype=torch.float32)

    # Randomly pair up the X- and Y-components
    y_vec = y_vec[torch.randperm(n)]

    # Combine the paired up components into vectors
    vec = torch.stack((x_vec, y_vec), dim=1)

    # Sort the vectors by angle
    angles = torch.atan2(vec[:, 1], vec[:, 0])
    _, indices = torch.sort(angles)
    vec = vec[indices]

    # Lay them end-to-end
    x, y = 0, 0
    min_polygon_x, min_polygon_y = 0, 0
    points = []

    for vx, vy in vec:
        points.append((x, y))
        x += vx.item()
        y += vy.item()
        min_polygon_x = min(min_polygon_x, x)
        min_polygon_y = min(min_polygon_y, y)

    # Move the polygon to the original min and max coordinates
    x_shift = min_x.item() - min_polygon_x
    y_shift = min_y.item() - min_polygon_y

    points = [(p[0] + x_shift, p[1] + y_shift) for p in points]
    points.append(points[0]) # Add the first point to close the polygon

    points_tensor = torch.tensor(points, device=device, dtype=torch.float32) # Convert to tensor
    if center_point is not None:
        # Shift the polygon to the center point
        points_tensor = points_tensor - torch.tensor([x_range / 2, y_range / 2], device=device, dtype=torch.float32) + center_point
        
    return points_tensor
            
def generate_dynamic_obstacles(start_pos: torch.Tensor, end_pos = torch.Tensor, vel: float = 0.4, distance_interval: float = 1.0, max_steps: int = 256, dt: float = 0.1):
    """This function generates a series of dynamics obstacles moving vertically downwards."""
    simulation_duration = max_steps * dt # Total duration of the simulation
    interval_duration = distance_interval / vel # Interval in second that the next obstacle starts to move so that the distance interval between two successive obstacles equals `distance_interval`
    n_obstacles = round(simulation_duration / interval_duration) # The number of obstacles needed for the whole simulation
    
    vel_traj = torch.zeros((n_obstacles, max_steps, 2))
    
    rot_traj = torch.zeros((n_obstacles, max_steps))
    rot_traj[:] = torch.deg2rad(torch.tensor(-90))
    
    pos_traj = start_pos.repeat(n_obstacles, max_steps, 1)
    
    for step_i in range(max_steps):
        t = step_i * dt
        for obs_j in range(n_obstacles):
            t_move = interval_duration * obs_j # The time point that the jth obstacle starts to move
            if t >= t_move:
                pos_y = pos_traj[obs_j, step_i, 1] - (t - t_move) * vel
                if pos_y >= end_pos[1]:
                    pos_traj[obs_j, step_i, 1] = pos_y # Do not exceed the end position
                    vel_traj[obs_j, step_i, 1] = -vel
                else:
                    pos_traj[obs_j, step_i, 1] = end_pos[1]
                    
    return pos_traj, vel_traj, rot_traj, n_obstacles

class ScenarioObstacleAvoidance(BaseScenario):
    def make_world(self, batch_dim: int, device: torch.device, **kwargs):
        # Specify parameters if not given
        if not hasattr(self, "parameters"):
            self.parameters = Parameters(
                is_testing_mode=is_testing_mode,
                is_visualize_short_term_path=is_visualize_short_term_path,
                max_steps=max_steps,
                is_dynamic_goal_reward=is_dynamic_goal_reward,
                obstacle_type="static", # Default type
                is_global_coordinate_sys=is_global_coordinate_sys,
                n_nearing_obstacles_observed=n_nearing_obstacles_observed,
                is_observe_corners=is_observe_corners,
                is_use_intermediate_goals=is_use_intermediate_goals,
            )
            
        if self.parameters.obstacle_type == "static":
            self.parameters.is_observe_corners = True
            print("The corners of static obstacles will be observed.")
            
        self.timer = Timer( # Timer for the first env
            start=time.time(),
            end=0,
            step=torch.zeros(batch_dim, device=device, dtype=torch.int),
            step_duration=torch.zeros(self.parameters.max_steps, device=device, dtype=torch.float32),
            step_begin=time.time(),
            render_begin=0,
        )
        
        print(f"Start time: {self.timer.start}.")
        n_agents_received = kwargs.get("n_agents", n_agents)
        if n_agents_received != 1:
            print(f"In path-tracking scenarios, (only) one agent is needed (but received '{n_agents_received})'.")
        
        self.n_agents = 1 # Only one agent is needed in the path-tracking scenarios
            
        width = kwargs.get("width", agent_width) # Agent width
        l_f = kwargs.get("l_f", agent_wheelbase_front) # Distance between the front axle and the center of gravity
        l_r = kwargs.get("l_r", agent_wheelbase_rear) # Distance between the rear axle and the center of gravity
        max_steering_angle = kwargs.get("max_steering_angle", torch.tensor(agent_max_steering_angle, device=device, dtype=torch.float32).deg2rad())
        max_speed = kwargs.get("max_speed", torch.tensor(agent_max_speed))

        self.corners = torch.zeros((batch_dim, 5, 2), device=device, dtype=torch.float32) 
        
        if self.parameters.obstacle_type == "static":
            x_range_obs = 1.5
            y_range_obs = 1.5
            random_convex_polygon = generate_random_convex_polygon(
                n=10, 
                x_range=x_range_obs,
                y_range=y_range_obs,
                center_point=torch.tensor([0, 0], device=device, dtype=torch.float32),
                device=device,
            )            

            self.is_line_obstacle = True # TODO 
            if self.is_line_obstacle:
                line_obstacle = torch.tensor([[0, 0.2], [0, -0.2]], device=device, dtype=torch.float32)
                
            self.obstacles = Obstacles(
                n=1,
                pos=torch.tensor([[0, 0]] , device=device, dtype=torch.float32),
                rot=torch.tensor([0] , device=device, dtype=torch.float32),
                vel=torch.tensor([[0, 0]] , device=device, dtype=torch.float32),
                corners=random_convex_polygon if (not self.is_line_obstacle) else line_obstacle,
            )
        else:
            n_corners_obstacle = 4
            x_range_obs = 0
            y_range_obs = 4
            start_pos = torch.tensor([x_range_obs/2, y_range_obs/2] , device=device, dtype=torch.float32)
            end_pos = torch.tensor([x_range_obs/2, -y_range_obs/2] , device=device, dtype=torch.float32)
            pos_traj, vel_traj, rot_traj, n_obstacles = generate_dynamic_obstacles(
                start_pos=start_pos, 
                end_pos=end_pos,
                vel=0.4, 
                distance_interval=1.0, # [m]
                max_steps=self.parameters.max_steps, 
                dt=dt,
            )
            
            self.obstacles = Obstacles(
                n=n_obstacles,
                pos=pos_traj,
                corners=torch.zeros((n_obstacles, self.parameters.max_steps, n_corners_obstacle + 1, 2), device=device, dtype=torch.float32),
                vel=vel_traj,
                rot=rot_traj,
                length=0.15,
                width=0.15,
            )
            
            # Get the corners of the obstacles at each time step
            for i_obs in range(self.obstacles.n):
                self.obstacles.corners[i_obs] = get_rectangle_corners(
                    center=pos_traj[i_obs],
                    yaw=rot_traj[i_obs].unsqueeze(1),
                    width=0.1,
                    length=0.1,
                    is_close_shape=True
                )
              
              
        tracking_path, ranges_path, self.start_pos, self.start_rot, self.start_vel, self.goal_pos, self.goal_rot, is_ref_path_loop, point_extended = get_ref_path_for_obstacle_avoidance_scenarios(
            agent_width=agent_width,
            agent_length=agent_length,
            point_interval=0.1,
            max_speed=max_speed,
            device=device,
            center_point=torch.tensor([0, 0], device=device, dtype=torch.float32), 
            is_visualize=False,
            is_save_fig=False,
            obstacles=self.obstacles.corners
        )
        
        
        world_x_dim = max(max(ranges_path), x_range_obs, y_range_obs) + 5 * agent_length
        world_y_dim = world_x_dim
        
        self.is_currently_at_goal = torch.zeros(batch_dim, device=device, dtype=torch.bool) # If agents currently are at their goal positions
        self.has_reached_goal  = torch.zeros(batch_dim, device=device, dtype=torch.bool) # Record goal-reaching status
        self.is_leave_world  = torch.zeros(batch_dim, device=device, dtype=torch.bool) # Record if the agent leaves the world
  
        _, lengths_path, _, vecs_path = get_center_length_yaw_polyline(polyline=tracking_path)
        
        vecs_path_norm = vecs_path / lengths_path.unsqueeze(1)
        
        # Define reference paths
        self.ref_paths = ReferencePathsMapRelated(
            long_term=tracking_path.unsqueeze(0), # Long-term reference path
            long_term_vecs_normalized=vecs_path_norm,
            is_ref_path_loop=is_ref_path_loop,
            point_extended=point_extended,
            n_points_short_term=torch.tensor(n_points_short_term, device=device, dtype=torch.int),
            short_term=torch.zeros((batch_dim, self.n_agents, n_points_short_term, 2), device=device, dtype=torch.float32), # Short-term reference path
            short_term_indices = torch.zeros((batch_dim, self.n_agents, n_points_short_term), device=device, dtype=torch.int),
        )
        
        self.normalizers = Normalizers(
            pos=torch.hstack((world_x_dim, world_y_dim)),
            v=max_speed,
            yaw=torch.tensor(2 * torch.pi, device=device, dtype=torch.float32),
            steering=max_steering_angle,
        )
        
        weighting_ref_directions = torch.linspace(1, 0.2, steps=self.ref_paths.n_points_short_term-1, device=device, dtype=torch.float32)
        weighting_ref_directions /= weighting_ref_directions.sum()
        self.rewards = Rewards(
            progress=torch.tensor(reward_progress, device=device, dtype=torch.float32),
            weighting_ref_directions=weighting_ref_directions, # Progress in the weighted directions (directions indicating by closer short-term reference points have higher weights)
            higth_v=torch.tensor(reward_speed, device=device, dtype=torch.float32),
            reach_goal=torch.tensor(reward_reach_goal, device=device, dtype=torch.float32),
            reach_intermediate_goal=torch.tensor(reward_reach_intermediate_goals, device=device, dtype=torch.float32),
        )

        self.penalties = Penalties(
            deviate_from_ref_path=torch.tensor(penalty_deviate_from_ref_path, device=device, dtype=torch.float32),
            deviate_from_goal=torch.tensor(penalty_deviate_from_goal, device=device, dtype=torch.float32),
            weighting_deviate_from_ref_path=torch.tensor(agent_width, device=device, dtype=torch.float32),
            leave_world=torch.tensor(penalty_leave_world, device=device, dtype=torch.float32),
            time=torch.tensor(penalty_time, device=device, dtype=torch.float32),
            change_steering=torch.tensor(penalty_change_steering, device=device, dtype=torch.float32),
            collide_with_obstacles=torch.tensor(penalty_collide_with_obstacles, device=device, dtype=torch.float32),
        )
        
        self.observations = Observations(
            is_local=torch.tensor(is_local_observation, device=device, dtype=torch.bool),
            is_global_coordinate_sys=torch.tensor(self.parameters.is_global_coordinate_sys, device=device, dtype=torch.bool),
            is_add_noise=torch.tensor(is_add_noise, device=device, dtype=torch.bool),
            noise_level=torch.tensor(noise_level, device=device, dtype=torch.float32),
            n_stored_steps=torch.tensor(n_stored_steps, device=device, dtype=torch.int),
            n_observed_steps=torch.tensor(n_observed_steps, device=device, dtype=torch.int),
            is_observe_corners=torch.tensor(self.parameters.is_observe_corners, device=device, dtype=torch.bool),
            n_nearing_obstacles_observed=torch.tensor(self.parameters.n_nearing_obstacles_observed, device=device, dtype=torch.int),
        )
        assert self.observations.n_stored_steps >= 1, "The number of stored steps should be at least 1."
        assert self.observations.n_observed_steps >= 1, "The number of observed steps should be at least 1."
        assert self.observations.n_stored_steps >= self.observations.n_observed_steps, "The number of stored steps should be greater or equal than the number of observed steps."
        
        self.observations.past_pos = torch.zeros((batch_dim, self.observations.n_stored_steps, 2), device=device, dtype=torch.float32) # Past + current
        self.observations.past_vel = torch.zeros((batch_dim, self.observations.n_stored_steps), device=device, dtype=torch.float32) # Past + current
        self.observations.past_action_vel = torch.zeros((batch_dim, self.observations.n_stored_steps), device=device, dtype=torch.float32)
        self.observations.past_action_steering = torch.zeros((batch_dim, self.observations.n_stored_steps), device=device, dtype=torch.float32)
        self.observations.past_distance_to_ref_path = torch.zeros((batch_dim, self.observations.n_stored_steps), device=device, dtype=torch.float32)
        
        # Distances to boundaries and reference path, and also the closest point on the reference paths of agents
        self.distances = Distances(
            ref_paths=torch.zeros((batch_dim, self.n_agents), device=device, dtype=torch.float32),
            closest_point_on_ref_path=torch.zeros((batch_dim, self.n_agents), device=device, dtype=torch.int),
            goal=torch.zeros((batch_dim, self.n_agents), device=device, dtype=torch.float32),
            obstacles=torch.zeros((batch_dim, self.obstacles.n), device=device, dtype=torch.float32),
        )
        
        self.thresholds = Thresholds(
            reach_goal=torch.tensor(threshold_reach_goal, device=device, dtype=torch.float32),
            reach_intermediate_goal=torch.tensor(threshold_reach_intermediate_goal, device=device, dtype=torch.float32),
            change_steering=torch.tensor(threshold_change_steering, device=device, dtype=torch.float32).deg2rad(),
        )
        
        self.collisions = Collisions(
            with_obstacles=torch.zeros((batch_dim, self.obstacles.n), device=device, dtype=torch.bool),
        )
        
        if self.parameters.is_dynamic_goal_reward:
            self.evaluation = Evaluation(
                pos_traj=torch.zeros((batch_dim, self.n_agents, self.parameters.max_steps, 2), device=device, dtype=torch.float32),
                v_traj=torch.zeros((batch_dim, self.n_agents, self.parameters.max_steps, 2), device=device, dtype=torch.float32),
                rot_traj=torch.zeros((batch_dim, self.n_agents, self.parameters.max_steps), device=device, dtype=torch.float32),
                deviation_from_ref_path=torch.zeros((batch_dim, self.n_agents, self.parameters.max_steps), device=device, dtype=torch.float32),
                path_tracking_error_mean=torch.zeros((batch_dim, self.n_agents), device=device, dtype=torch.float32),
            )
            
                
        # Store the index of the nearest point on the reference path, which indicates the moving progress of the agent
        self.progress = torch.zeros((batch_dim, self.n_agents), device=device)
        self.progress_previous = torch.zeros((batch_dim, self.n_agents), device=device)

        # Store the reaching status of intermediate goals
        self.are_intermediate_goals_reached = torch.zeros((batch_dim, self.n_agents, self.ref_paths.long_term.shape[1]), device=device, dtype=torch.bool)
        
        # Make world
        world = World(
            batch_dim, 
            device, 
            x_semidim=torch.tensor(world_x_dim, device=device, dtype=torch.float32),
            y_semidim=torch.tensor(world_y_dim, device=device, dtype=torch.float32),
            dt=dt
        )
        # world._drag = 0 # !No drag/friction

        # Use the kinematic bicycle model for the agent
        agent = Agent(
            name=f"agent_0",
            shape=Box(length=l_f+l_r, width=width),
            collide=False,
            render_action=False,
            u_range=[max_speed, max_steering_angle], # Control command serves as velocity command 
            u_multiplier=[1, 1],
            max_speed=max_speed,
            dynamics=KinematicBicycle(
                world, 
                width=width, 
                l_f=l_f, 
                l_r=l_r, 
                max_steering_angle=max_steering_angle, 
                integration="rk4" # one of "euler", "rk4"
            )
        )
        # Create a variable to store the position of the agent at the previous time step 
        agent.state.pos_previous = torch.zeros((batch_dim, 2), device=device, dtype=torch.float32)
        world.add_agent(agent)

        return world

    def reset_world_at(self, env_index: int = None):
        # print(f"[DEBUG] reset_world_at(): env_index = {env_index}")
        """
        This function resets the world at the specified env_index.

        Args:
        :param env_index: index of the environment to reset. If None a vectorized reset should be performed

        """
        i = 0 # Only one agent
        agent = self.world.agents[i]
        
        # if hasattr(self, 'training_info'):
        #     print(self.training_info["agents"]["episode_reward"].mean())
        
        # TODO Reset the reference path according to the training progress
        # self.ref_paths.long_term = get_reference_paths(self.n_agents, self.map_data) # A new random long-term reference path

        agent.set_pos(self.start_pos, batch_index=env_index)
        agent.set_rot(self.start_rot, batch_index=env_index)
        agent.set_vel(self.start_vel, batch_index=env_index)

        if (env_index is None) or (env_index == 0):
            self.timer.step_duration[:] = 0
            self.timer.start = time.time()
            self.timer.step_begin = time.time()
            self.timer.end = 0
                
        # Reset variables for the agent
        if env_index is None: # Reset all envs
            
            self.is_currently_at_goal[:] = False
            self.has_reached_goal[:] = False
            self.is_leave_world[:] = False
            
            # Reset distances to the reference path          
            self.distances.ref_paths[:,i], self.distances.closest_point_on_ref_path[:,i] = get_perpendicular_distances(
                point=agent.state.pos, 
                polyline=self.ref_paths.long_term[i]
            )
            
            self.distances.goal[:,i] = (agent.state.pos - self.goal_pos).norm(dim=1)
            
            self.distances.obstacles # TODO

            self.ref_paths.short_term[:,i], self.ref_paths.short_term_indices[:,i] = get_short_term_reference_path( 
                reference_path=self.ref_paths.long_term[i],
                closest_point_on_ref_path=self.distances.closest_point_on_ref_path[:,i],
                n_points_short_term=self.ref_paths.n_points_short_term, 
                device=self.world.device,
                is_ref_path_loop=self.ref_paths.is_loop,
                point_extended=self.ref_paths.point_extended,
            )
            
            if self.parameters.is_dynamic_goal_reward:
                # Reset the data for evaluation for all envs
                self.evaluation.pos_traj[:] = 0
                self.evaluation.v_traj[:] = 0
                self.evaluation.rot_traj[:] = 0
                self.evaluation.path_tracking_error_mean[:] = 0

            # Reset the previous position
            agent.state.pos_previous = agent.state.pos.clone()
            
            # Reset the reaching status of intermediate goals for all envs
            self.are_intermediate_goals_reached[:] = False
            
            self.observations.past_pos[:] = 0
            self.observations.past_vel[:] = 0
            self.observations.past_action_vel[:] = 0
            self.observations.past_action_steering[:] = 0
            self.observations.past_distance_to_ref_path[:] = 0
            
            self.corners[:] = get_rectangle_corners(
                center=agent.state.pos, 
                yaw=agent.state.rot, 
                width=agent.shape.width, 
                length=agent.shape.length,
                is_close_shape=True
            )
            
            self.collisions.with_obstacles[:] = 0

            # Store the index of the nearest point on the reference path, which indicates the moving progress of the agent
            # TODO Necessary?
            self.progress_previous[:,i] = self.progress[:,i].clone() # Store the values of previous time step 
            self.progress[:,i] = self.ref_paths.short_term_indices[:,i,0]
            
            self.timer.step[:] = 0
            
        else: # Reset the env specified by `env_index`
            self.is_currently_at_goal[env_index] = False
            self.has_reached_goal[env_index] = False
            self.is_leave_world[env_index] = False
            
            # Reset distances to the reference path          
            self.distances.ref_paths[env_index,i], self.distances.closest_point_on_ref_path[env_index,i] = get_perpendicular_distances(
                point=agent.state.pos[env_index,:].unsqueeze(0), 
                polyline=self.ref_paths.long_term[i]
            )
            
            self.distances.goal[env_index,i] = (agent.state.pos[env_index,:] - self.goal_pos.unsqueeze(0)).norm(dim=1)

            # Reset the short-term reference path of agents in env `env_index`
            self.ref_paths.short_term[env_index,i], self.ref_paths.short_term_indices[env_index,i] = get_short_term_reference_path( 
                reference_path=self.ref_paths.long_term[i], 
                closest_point_on_ref_path=self.distances.closest_point_on_ref_path[env_index,i].unsqueeze(0),
                n_points_short_term=self.ref_paths.n_points_short_term, 
                device=self.world.device,
                is_ref_path_loop=self.ref_paths.is_loop,
                point_extended=self.ref_paths.point_extended,
            )
            
            if self.parameters.is_dynamic_goal_reward:
                # Reset the data for evaluation for env `env_index`
                self.evaluation.pos_traj[env_index] = 0
                self.evaluation.v_traj[env_index] = 0
                self.evaluation.rot_traj[env_index] = 0
                self.evaluation.path_tracking_error_mean[env_index] = 0
            
            # Reset the previous position for env `env_index`
            agent.state.pos_previous[env_index,:] = agent.state.pos[env_index,:].clone()
        
            # Reset the reaching status of intermediate goals for env `env_index`
            self.are_intermediate_goals_reached[env_index, :] = False
            
            self.observations.past_pos[env_index, :] = 0
            self.observations.past_vel[env_index, :] = 0
            self.observations.past_action_vel[env_index, :] = 0
            self.observations.past_action_steering[env_index, :] = 0
            self.observations.past_distance_to_ref_path[env_index, :] = 0

            self.corners[env_index] = get_rectangle_corners(
                center=agent.state.pos[env_index].unsqueeze(0),
                yaw=agent.state.rot[env_index].unsqueeze(0),
                width=agent.shape.width, 
                length=agent.shape.length,
                is_close_shape=True
            )

            self.collisions.with_obstacles[env_index, :] = 0
            
            # Store the index of the nearest point on the reference path, which indicates the moving progress of the agent
            # TODO Necessary?
            self.progress_previous[env_index,i] = self.progress[env_index,i].clone() # Store the values of previous time step 
            self.progress[env_index,i] = self.ref_paths.short_term_indices[env_index,i,0]

            self.timer.step[env_index] = 0


    def reward(self, agent: Agent):
        # print("[DEBUG] reward()")
        agent_index = self.world.agents.index(agent) # Get the index of the current agent

        # Timer
        if agent_index == 0:
            self.timer.step_duration[self.timer.step] = time.time() - self.timer.step_begin                
            self.timer.step_begin = time.time() # Set to the current time as the begin of the current time step
            # Increment step by 1
            self.timer.step += 1

        # Update the corners of the agent
        self.corners[:] = get_rectangle_corners(
            center=agent.state.pos,
            yaw=agent.state.rot,
            width=agent.shape.width,
            length=agent.shape.length,
            is_close_shape=True,
        )
            
        # Initialize
        self.rew = torch.zeros(self.world.batch_dim, device=self.world.device, dtype=torch.float32)
        
        # Calculate the distance from the center of the agent to its reference path
        self.distances.ref_paths[:,agent_index], self.distances.closest_point_on_ref_path[:,agent_index] = get_perpendicular_distances(
            point=agent.state.pos, 
            polyline=self.ref_paths.long_term[agent_index]
        )

        ##################################################
        ## Penalty for deviating from reference path
        ##################################################
        self.rew += self.distances.ref_paths[:,agent_index] / self.penalties.weighting_deviate_from_ref_path * self.penalties.deviate_from_ref_path       
        
        ##################################################
        ## Reward for the actual movement
        ##################################################
        # TODO Check if this is necessary when there are rewards for intermediate goals
        movement = (agent.state.pos - agent.state.pos_previous) # Calculate the progress of the agent
        # Handle non-loop reference path
        len_long_term_ref_path = len(self.ref_paths.long_term[agent_index])
        short_term_indices = torch.where(self.ref_paths.short_term_indices == len_long_term_ref_path - 1, len_long_term_ref_path - 2, self.ref_paths.short_term_indices)
        short_term_path_vec_normalized = self.ref_paths.long_term_vecs_normalized[short_term_indices[:,agent_index, 0:-1]] # Narmalized vector of the short-term reference path
        movement_normalized_proj = torch.sum(movement.unsqueeze(1) * short_term_path_vec_normalized, dim=2)
        movement_normalized_proj = torch.where(movement_normalized_proj < 0, movement_normalized_proj * 2, movement_normalized_proj) # Penalize more if move backwards
        movement_weighted_sum_proj = torch.matmul(movement_normalized_proj, self.rewards.weighting_ref_directions)
        self.rew += movement_weighted_sum_proj / (agent.max_speed * self.world.dt) * self.rewards.progress # Relative to the maximum possible movement
        
        ##################################################
        ## Reward for intermediate goals
        ##################################################
        # All points on the reference path are considered "intermediate goals" 
        distances_to_intermediate_goals, is_projection_inside_line = get_point_line_distance(
            points=self.ref_paths.long_term[agent_index],
            lines_start_points=agent.state.pos_previous,
            lines_end_points=agent.state.pos # Use not only the current position but also the previous position to cover the case that the an intermediate goal lies between the previous and the current positions
        )
        # Update the reaching status of intermediate goals (a intermediate goal is only considered reached if its projection is inside the agent's trajectry)
        are_intermediate_goals_reached = (distances_to_intermediate_goals <= self.thresholds.reach_intermediate_goal) & is_projection_inside_line
        new_intermediate_goals_reached = (~self.are_intermediate_goals_reached[:, agent_index, :]) & are_intermediate_goals_reached
        # Update intermediate-goal-reaching status
        self.are_intermediate_goals_reached[:, agent_index, :] |= new_intermediate_goals_reached
        factor_intermediate_goals_reached = torch.zeros((self.world.batch_dim), device=self.world.device, dtype=torch.float32)
        for idx, dist in enumerate(distances_to_intermediate_goals):
            factor_intermediate_goals_reached[idx] = (self.thresholds.reach_intermediate_goal - (dist[new_intermediate_goals_reached[idx, :]]).mean()) / self.thresholds.reach_intermediate_goal
            factor_intermediate_goals_reached[factor_intermediate_goals_reached.isnan()] = 0
                    
        self.rew += new_intermediate_goals_reached.sum(dim=1) * self.rewards.reach_intermediate_goal * factor_intermediate_goals_reached
            
        
        ##################################################
        ## Reward for moving in a high velocity and the right direction
        ##################################################
        # TODO: optional?
        v_proj = torch.sum(agent.state.vel.unsqueeze(1) * short_term_path_vec_normalized, dim=2)
        v_proj_weighted_sum = torch.matmul(v_proj, self.rewards.weighting_ref_directions)
        self.rew += v_proj_weighted_sum / agent.max_speed * self.rewards.higth_v

        # Save previous positions
        agent.state.pos_previous = agent.state.pos.clone() 

        if not self.ref_paths.is_loop: 
            ##################################################
            ## Reward for reaching goal (only when reference path is not a loop)
            ##################################################
            # Update distances to goal positions
            self.distances.goal[:, agent_index] = distances_to_intermediate_goals[:, -1]

            is_currently_at_goal = are_intermediate_goals_reached[:, -1]
            goal_reward_factor = (self.thresholds.reach_goal- self.distances.goal[:, agent_index]) / self.thresholds.reach_goal
            
            self.rew += is_currently_at_goal * ~self.has_reached_goal * self.rewards.reach_goal * goal_reward_factor # Agents can only receive the goal reward once per iteration
            # Update goal-reaching status
            self.has_reached_goal[is_currently_at_goal] = True

            ##################################################
            ## Penalty for leaving goal position (not relevant to single-agent scenario)
            ##################################################
            # This penalty is only applied to agents that have reached their goal positions before and now leave them, aiming to encourage these agents to stay at their goal positions
            self.rew += (self.distances.goal[:, agent_index] - self.thresholds.reach_goal).clamp(min=0) * self.penalties.deviate_from_goal * self.has_reached_goal
        else:
            # Reset the intermediate goals if the reference path is a loop
            is_reset_intermediate_goals = self.are_intermediate_goals_reached.sum(dim=(1,2)) > 0.75 * self.ref_paths.long_term[agent_index].shape[0] # Resear only when a large amount of intermediate goals are reached
            self.are_intermediate_goals_reached[is_reset_intermediate_goals,:] = False
            # if is_reset_intermediate_goals.any():
            #     print(f"reset intermediate goals {is_reset_intermediate_goals}")

        ##################################################
        ## Penalty for leaving the world
        ##################################################
        is_leave_world = (
            (agent.state.pos[:, 0] < -self.world.x_semidim / 2) | 
            (agent.state.pos[:, 0] > self.world.x_semidim / 2) | 
            (agent.state.pos[:, 1] < -self.world.y_semidim / 2) | 
            (agent.state.pos[:, 1] > self.world.y_semidim / 2)
        )
        self.is_leave_world[is_leave_world] = True
        self.rew += self.is_leave_world * self.penalties.leave_world
        
        ##################################################
        ## Penalty for changing steering angle too much
        ##################################################
        steering_change = torch.clamp((self.observations.past_action_steering[:, -1] - self.observations.past_action_steering[:, -2]).abs() - self.thresholds.change_steering, min=0)
        steering_change_reward_factor = steering_change / (2 * agent.u_range[1] - self.thresholds.change_steering)
        self.rew += steering_change_reward_factor * self.penalties.change_steering
        # print(f"Penality for changing steering angle {steering_change_reward_factor * self.penalties.change_steering}")
        
        ##################################################
        ## Penalty for colliding with obstacles
        ##################################################
        self.collisions.with_obstacles[:] = False # Reset
        if self.parameters.obstacle_type == "dynamic":
            for obs_i in range(self.obstacles.n):
                is_collide_with_obstacles = interX(self.corners, self.obstacles.corners[obs_i, self.timer.step], False)
                self.collisions.with_obstacles[:, obs_i] |= is_collide_with_obstacles
        else:
            # Static obstacles
            self.collisions.with_obstacles[:] = interX(self.corners, self.obstacles.corners.repeat(self.world.batch_dim, 1, 1), False).unsqueeze(1)
            
        self.rew += self.collisions.with_obstacles.sum(dim=1) * self.penalties.collide_with_obstacles

        ##################################################
        ## Penalty for being too close to obstacles # TODO
        ##################################################
        # if self.parameters.obstacle_type == "dynamic":
        #     for obs_i in range(self.obstacles.n):
        #         self.distances.obstacles[obs_i] = get_distances_between_agents(
        #             point=agent.state.pos,
        #             polyline=self.obstacles.corners[obs_i, self.timer.step],
        #         )
        # else:
        #     self.distances.obstacles[:] = get_perpendicular_distances(
        #         point=agent.state.pos,
        #         polyline=self.obstacles.corners,
        #     )
        
        ##################################################
        ## Penalty for losing time
        ##################################################
        self.rew += self.penalties.time

        # Update the short-term reference path (extract from the long-term reference path)
        self.ref_paths.short_term[:,agent_index], self.ref_paths.short_term_indices[:,agent_index] = get_short_term_reference_path(
            reference_path=self.ref_paths.long_term[agent_index], 
            closest_point_on_ref_path=self.distances.closest_point_on_ref_path[:,agent_index],
            n_points_short_term=self.ref_paths.n_points_short_term, 
            device=self.world.device,
            is_ref_path_loop=self.ref_paths.is_loop,
            point_extended=self.ref_paths.point_extended,
        )
        
        # [not used] Store the index of the nearest point on the reference path, which indicates the moving progress of the agent
        # TODO: decide if this is necessary
        self.progress_previous[:,agent_index] = self.progress[:,agent_index].clone() # Store the values of previous time step 
        self.progress[:,agent_index] = self.ref_paths.short_term_indices[:,agent_index,0]
        
        return self.rew


    def observation(self, agent: Agent):
        # print("[DEBUG] observation()")
        """
        Generate an observation for the given agent in all envs.

        Parameters:
        - agent (Agent): The agent for which the observation is to be generated.

        Returns:
        - The observation for the given agent in all envs.
        """
        agent_index = self.world.agents.index(agent)
        batch_dim = self.world.batch_dim

        if self.observations.is_add_noise:
            # Generate random noise from a standard normal distribution N(0, 1). Multiply a factor to adjust the noice level
            noise_ref_path = torch.randn(batch_dim, self.ref_paths.n_points_short_term, 2) * self.observations.noise_level #  torch.randn_like generates ramdom noise with the same shape as the input
            noise_distances_to_ref_path = torch.randn(batch_dim) * self.observations.noise_level
        else:
            noise_ref_path = 0
            noise_distances_to_ref_path = 0

        if self.observations.is_global_coordinate_sys:
            positions = torch.stack([a.state.pos for a in self.world.agents], dim=1) / self.normalizers.pos
            velocities = torch.stack([a.state.vel for a in self.world.agents], dim=1) / self.normalizers.v
            ##################################################
            ## Observation of intermediate/final goal
            ##################################################
            if self.parameters.is_use_intermediate_goals:
                # Skip the first point on the short-term reference path, because, mostly, it is behind the agent. The second point is in front of the agent.
                obs_ref_point_norm = ((self.ref_paths.short_term[:,agent_index] + noise_ref_path) / self.normalizers.pos).reshape(batch_dim, -1)
            else:
                obs_ref_point_norm = self.goal_pos.repeat(batch_dim, 1)
        else:
            positions = torch.zeros((batch_dim, 1, 2)) # Positions are at the origin of the local coordinate system
            velocities = torch.stack([a.state.vel for a in self.world.agents], dim=1) / self.normalizers.v
            velocities[:,:,0] = velocities.norm(dim=-1)
            velocities[:,:,1] = 0
            
            ##################################################
            ## Observation of intermediate/final goal
            ##################################################
            if self.parameters.is_use_intermediate_goals:
                # Normalized short-term reference path relative to the agent's current position
                # Skip the first point on the short-term reference path, because, mostly, it is behind the agent. The second point is in front of the agent.
                ref_points_rel_norm = ((self.ref_paths.short_term[:,agent_index] + noise_ref_path) - agent.state.pos.unsqueeze(1)) / self.normalizers.pos
                ref_points_rel_abs = ref_points_rel_norm.norm(dim=2)
                ref_points_rel_rot = torch.atan2(ref_points_rel_norm[:,:,1], ref_points_rel_norm[:,:,0]) - agent.state.rot
                obs_ref_point_norm = torch.stack(
                    (
                        ref_points_rel_abs * torch.cos(ref_points_rel_rot), 
                        ref_points_rel_abs * torch.sin(ref_points_rel_rot)    
                    ), dim=2
                ).reshape(batch_dim, -1)
            else:
                goal_vec = (self.goal_pos - agent.state.pos) / self.normalizers.pos
                goal_vec_norm = goal_vec.norm(dim=1).unsqueeze(1)
                goal_rot_rel = torch.atan2(goal_vec[:, 1], goal_vec[:, 0]).unsqueeze(1) - agent.state.rot
                obs_ref_point_norm = torch.stack(
                    (
                        goal_vec_norm * torch.cos(goal_rot_rel), 
                        goal_vec_norm * torch.sin(goal_rot_rel)    
                    ), dim=2
                ).reshape(batch_dim, -1)
                

        empty_actions = torch.zeros( # Will be used in case that actions are None at the begining
            (batch_dim, agent.action.action_size), device=self.world.device, dtype=torch.float32
        )
        
        ##################################################
        ## Observation of positions
        ##################################################
        # Shift old observations by one step and add the current observations
        self.observations.past_pos[:, 0:-1] = self.observations.past_pos[:, 1:].clone()
        self.observations.past_pos[:, -1] = positions[:, agent_index].clone()
        
        ##################################################
        ## Observation of velocities
        ##################################################
        # Shift old observations by one step and add the current observations
        self.observations.past_vel[:, 0:-1] = self.observations.past_vel[:, 1:].clone()
        self.observations.past_vel[:, -1] = velocities[:, agent_index, 0].clone()
        
        ##################################################
        ## Observation of actions
        ##################################################
        self.observations.past_action_vel[:, 0:-1] = self.observations.past_action_vel[:, 1:].clone()
        self.observations.past_action_vel[:, -1] = (agent.action.u[:, 0] if (agent.action.u is not None) else empty_actions[:, 0]).clone()
        
        self.observations.past_action_steering[:, 0:-1] = self.observations.past_action_steering[:, 1:].clone()
        self.observations.past_action_steering[:, -1] = (agent.action.u[:, 1] if (agent.action.u is not None) else empty_actions[:, 0]).clone()
        
        ##################################################
        ## Observation of distance to reference path
        ##################################################
        self.observations.past_distance_to_ref_path[:, 0:-1] = self.observations.past_distance_to_ref_path[:, 1:].clone()
        self.observations.past_distance_to_ref_path[:, -1] = (self.distances.ref_paths[:,agent_index] + noise_distances_to_ref_path).clone()

        ##################################################
        ## Observation of obstacles
        ##################################################
        if self.observations.is_global_coordinate_sys:
            # TODO
            if "dynamic" in self.parameters.obstacle_type:
                obs_corners = self.obstacles.corners[:, self.timer.step, 0:4, :].transpose(0, 1) / self.normalizers.pos
                obs_velocities = self.obstacles.vel[:, self.timer.step, :].transpose(0, 1) / self.normalizers.v
            else:
                obs_corners = self.obstacles.corners / self.normalizers.pos
                obs_velocities = None
        else:
            if "dynamic" in self.parameters.obstacle_type:
                if self.observations.n_nearing_obstacles_observed is not None:
                    # local coordinate & dynamic obstacles & observe partial obstacles
                    # Initialize
                    obs_corners = torch.zeros((batch_dim, self.observations.n_nearing_obstacles_observed, 4, 2), device=self.world.device, dtype=torch.float32)
                    obs_positions = torch.zeros((batch_dim, self.observations.n_nearing_obstacles_observed, 2), device=self.world.device, dtype=torch.float32)
                    obs_rotations = torch.zeros((batch_dim, self.observations.n_nearing_obstacles_observed, 1), device=self.world.device, dtype=torch.float32)
                    obs_velocities = torch.zeros((batch_dim, self.observations.n_nearing_obstacles_observed, 2), device=self.world.device, dtype=torch.float32)
                    # Find the nearest obstacles
                    dis_2_obstacles = (agent.state.pos.unsqueeze(1) - self.obstacles.pos[:, self.timer.step].transpose(0,1)).norm(dim=-1)
                    _, nearing_obstacles_indices = torch.topk(
                        dis_2_obstacles, 
                        k=min(self.observations.n_nearing_obstacles_observed, self.obstacles.n), # Consider the case where the number of obstacles to be observed exceeds the total number of obstacles. In this case, "placeholder obstacles" will be used to keep the observation size
                        largest=False
                    )
                    for nearing_obs_i in range(nearing_obstacles_indices.shape[1]):
                        nearing_obs_idx = nearing_obstacles_indices[:, nearing_obs_i]
                        
                        corners_nearing_obs = []
                        pos_nearing_obs = []
                        rot_nearing_obs = []
                        vel_nearing_obs = []
                        for env_idx, obs_idx in enumerate(nearing_obs_idx):
                            corners_nearing_obs.append(self.obstacles.corners[obs_idx, self.timer.step[env_idx]])
                            pos_nearing_obs.append(self.obstacles.pos[obs_idx, self.timer.step[env_idx]])
                            rot_nearing_obs.append(self.obstacles.rot[obs_idx, self.timer.step[env_idx]])
                            vel_nearing_obs.append(self.obstacles.vel[obs_idx, self.timer.step[env_idx]])
                        
                        # Get the relative positions of the corners of the obstacles
                        obs_corners[:, nearing_obs_i] = transform_from_global_to_local_coordinate(
                            pos_i=agent.state.pos,
                            pos_j=torch.stack(corners_nearing_obs, dim=0),
                            rot_i=agent.state.rot,
                        )
                        obs_positions[:, nearing_obs_i] = torch.vstack(pos_nearing_obs) - agent.state.pos
                        
                        # Get the relative velocities of the obstacles
                        obs_rotations[:, nearing_obs_i] = torch.vstack(rot_nearing_obs) - agent.state.rot
                        obs_vel_norm = torch.vstack(vel_nearing_obs).norm(dim=1).unsqueeze(1)
                        obs_velocities[:, nearing_obs_i] = torch.hstack(
                            (
                                obs_vel_norm * torch.cos(obs_rotations[:, nearing_obs_i]), 
                                obs_vel_norm * torch.sin(obs_rotations[:, nearing_obs_i])
                            )
                        )
                else:
                    # local coordinate & dynamic obstacles & observe all obstacles
                    obs_corners = torch.zeros((batch_dim, self.obstacles.n, 4, 2), device=self.world.device, dtype=torch.float32)
                    obs_positions = torch.zeros((batch_dim, self.obstacles.n, 2), device=self.world.device, dtype=torch.float32)
                    obs_rotations = torch.zeros((batch_dim, self.obstacles.n, 1), device=self.world.device, dtype=torch.float32)
                    obs_velocities = torch.zeros((batch_dim, self.obstacles.n, 2), device=self.world.device, dtype=torch.float32)
                        
                    for obs_i in range(self.obstacles.n):
                        # Get the relative positions of the corners of the obstacles
                        obs_corners[:, obs_i] = transform_from_global_to_local_coordinate(
                            pos_i=agent.state.pos,
                            pos_j=self.obstacles.corners[obs_i, self.timer.step],
                            rot_i=agent.state.rot,
                        )
                        obs_positions[:, obs_i] = self.obstacles.pos[obs_i, self.timer.step] - agent.state.pos
                        # Get the relative velocities of the obstacles
                        obs_rotations[:, obs_i] = self.obstacles.rot[obs_i, self.timer.step].unsqueeze(1) - agent.state.rot
                        obs_vel_norm = self.obstacles.vel[obs_i, self.timer.step].norm(dim=1).unsqueeze(1)
                        obs_velocities[:, obs_i] = torch.hstack(
                            (
                                obs_vel_norm * torch.cos(obs_rotations[:, obs_i]), 
                                obs_vel_norm * torch.sin(obs_rotations[:, obs_i])
                            )
                        )
                        
                # Normalize
                obs_corners /= self.normalizers.pos
                if (self.observations.n_nearing_obstacles_observed is not None) and (self.observations.n_nearing_obstacles_observed > self.obstacles.n):
                    obs_corners[:, self.obstacles.n:] = 1 # "placeholder obstacles" are at the upper right edge of the world
                    
                obs_positions /= self.normalizers.pos
                obs_rotations = normalize_angle(obs_rotations) # Normalize to [-pi, pi]
                obs_rotations /= self.normalizers.yaw
                obs_velocities /= self.normalizers.v
            else:
                # local coordinate & static obstacles
                obs_corners = self.obstacles.corners.repeat(batch_dim, 1, 1)
                obs_positions = self.obstacles.pos.repeat(batch_dim, 1, 1)
                obs_rotations = normalize_angle(self.obstacles.rot - agent.state.rot)
                obs_velocities = self.obstacles.vel.repeat(batch_dim, 1)
                
        index_start = self.observations.n_stored_steps-self.observations.n_observed_steps
        
        obs_list = [
            self.observations.past_vel[:, index_start:] / self.normalizers.v,
            self.observations.past_action_vel[:, index_start:] / self.normalizers.v,
            self.observations.past_action_steering[:, index_start:] / self.normalizers.steering,
            self.observations.past_distance_to_ref_path[:, index_start:] / self.normalizers.pos.norm(),
            obs_ref_point_norm, # torch.Size([batch_dim, num_points])
            obs_corners.reshape(batch_dim, -1) if self.observations.is_observe_corners else obs_positions.reshape(batch_dim, -1),
        ]
        
        if not self.observations.is_observe_corners:
            # Rotation information should be included if corners are not observed 
            obs_list.append(
                obs_rotations.reshape(batch_dim, -1)
            )
            
        if self.observations.is_global_coordinate_sys:
            obs_list.insert(
                0,
                (self.observations.past_pos[:, index_start:] / self.normalizers.pos).reshape(batch_dim, -1)
            )
        
        if self.parameters.obstacle_type == "dynamic":
            obs_list.append(
                obs_velocities.reshape(batch_dim, -1)
            )
            
        return torch.hstack(obs_list)

    def info(self, agent: Agent) -> Dict[str, Tensor]:
        """
        This function computes the info dict for 'agent' in a vectorized way
        The returned dict should have a key for each info of interest and the corresponding value should
        be a tensor of shape (n_envs, info_size)

        Implementors can access the world at 'self.world'

        To increase performance, tensors created should have the device set, like:
        torch.tensor(..., device=self.world.device)

        :param agent: Agent batch to compute info of
        :return: info: A dict with a key for each info of interest, and a tensor value  of shape (n_envs, info_size)
        """
        agent_index = self.world.agents.index(agent) # Index of the current agent
        
        info = {
            "pos": agent.state.pos,
            "vel": agent.state.vel,
            "rot": agent.state.rot,
            "deviation_from_ref_path": self.distances.ref_paths[:,agent_index],
        }
        
        return info
    
    def done(self):
        # print("[DEBUG] done()")
        if self.ref_paths.is_loop:
            # Scenarios with reference path being a loop has no final goal
            is_done = torch.zeros((self.world.batch_dim), device=self.world.device, dtype=torch.bool) # [batch_dim]
        else:
            is_done = self.has_reached_goal.clone() # [batch_dim]
            if self.has_reached_goal.any():
                print("Reach goal!")
                
        if self.is_leave_world.any():
            print("Leave the world!")
        
        is_max_steps_reached = self.timer.step == self.parameters.max_steps - 1
        if is_max_steps_reached.any():
            print("The number of the maximum steps is reached.")
            
            
        is_collide_with_obstacles = self.collisions.with_obstacles.any(dim=1)
        if is_collide_with_obstacles.any():
            print("Collide with obstacles.")
            
        # Done if agents leave the world or if the maximum steps are reached
        is_done = is_done | self.is_leave_world | is_max_steps_reached | is_collide_with_obstacles
        
        
        if self.parameters.is_testing_mode:
            # print(f"Step: {self.timer.step}")
            is_done = torch.zeros(is_done.shape, device=self.world.device, dtype=torch.bool)
            if self.timer.step[0] % 20 == 0:
                print("You are in testing mode. Collisions do not terminate the simulation.")
        
        self.timer.end = time.time()
        # if is_done[0]:
            # print(f"Simulation duration: {self.timer.end - self.timer.start} sec.")
        
        return is_done
    
    def extra_render(self, env_index: int = 0):
        from vmas.simulator import rendering

        if self.parameters.is_real_time_rendering:
            if self.timer.step[0] == 0:
                pause_duration = 0 # Not sure how long should the simulation be paused at time step 0, so rather 0
            else:
                pause_duration = self.world.dt - (time.time() - self.timer.render_begin)
            if pause_duration > 0:
                time.sleep(pause_duration)
            # print(f"Paused for {pause_duration} sec.")
            
            self.timer.render_begin = time.time() # Update


        geoms = []
        
        # Visualize reference path
        geom = rendering.PolyLine(
            v = self.ref_paths.long_term[0],
            close=False,
        )
        
        xform = rendering.Transform()
        geom.add_attr(xform)
        geom.set_color(*Color.black100)
        geoms.append(geom)
                
        # Visualize goal
        if not self.ref_paths.is_loop:
            color = Color.red100
            circle = rendering.make_circle(radius=self.thresholds.reach_goal, filled=True)
            xform = rendering.Transform()
            circle.add_attr(xform)
            xform.set_translation(self.goal_pos[0], self.goal_pos[1])
            circle.set_color(*color)
            geoms.append(circle)

        # Visualize short-term reference path
        if self.parameters.is_visualize_short_term_path:
            geom = rendering.PolyLine(
                v = self.ref_paths.short_term[0, 0],
                close=False,
            )
            
            xform = rendering.Transform()
            geom.add_attr(xform)            
            geom.set_color(*Color.green100)
            geoms.append(geom)
                        
        # Visualize obstacles
        if self.parameters.obstacle_type == "static":
            geom = rendering.FilledPolygon(
                v = self.obstacles.corners,
            )
            
            xform = rendering.Transform()
            geom.add_attr(xform)            
            geom.set_color(*Color.blue50)
            geoms.append(geom)

        elif self.parameters.obstacle_type == "dynamic":
            corners = self.obstacles.corners[:, self.timer.step[env_index]]
            for i_c in corners:                
                geom = rendering.FilledPolygon(
                    v = i_c,
                )

                xform = rendering.Transform()
                
                geom.add_attr(xform)            
                geom.set_color(*Color.blue50)
                geoms.append(geom)
            
        return geoms

    def process_action(self, agent: Agent):
        """This function will be executed before the step function, i.e., states are not updated yet."""
        # print("[DEBUG] process_action()")
        agent_index = self.world.agents.index(agent)
        
        # Update data for evaluation
        if self.parameters.is_dynamic_goal_reward:
            self.evaluation.pos_traj[:,agent_index,self.timer.step] = agent.state.pos
            self.evaluation.v_traj[:,agent_index,self.timer.step] = agent.state.vel
            self.evaluation.rot_traj[:,agent_index,self.timer.step] = agent.state.rot.squeeze(1)
            self.evaluation.deviation_from_ref_path[:,agent_index,self.timer.step] = self.distances.ref_paths[:,agent_index]
            self.evaluation.path_tracking_error_mean[:,agent_index] = self.evaluation.deviation_from_ref_path[:,agent_index].sum(dim=1) / (self.timer.step + 1) # Step starts from 0
            
        if hasattr(agent, 'dynamics') and hasattr(agent.dynamics, 'process_force'):
            agent.dynamics.process_force()
        else:
            # The agent does not have a dynamics property, or it does not have a process_force method
            pass
        
if __name__ == "__main__":
    scenario_name = "car_like_robots_path_tracking" # car_like_robots_road_traffic, car_like_robots_go_to_formation, car_like_robots_path_tracking
    parameters = Parameters(
        n_agents=4,
        dt=0.1, # [s] sample time 
        device="cpu" if not torch.backends.cuda.is_built() else "cuda:0",  # The divice where learning is run
        scenario_name=scenario_name,
        
        # Training parameters
        n_iters=100, # Number of sampling and training iterations (on-policy: rollouts are collected during sampling phase, which will be immediately used in the training phase of the same iteration),
        frames_per_batch=2**10, # Number of team frames collected per training iteration (minibatch_size*10)
        num_epochs=30, # Number of optimization steps per training iteration,
        minibatch_size=2*9, # Size of the mini-batches in each optimization step (2**9 - 2**12?),
        lr=4e-4, # Learning rate,
        max_grad_norm=1.0, # Maximum norm for the gradients,
        clip_epsilon=0.2, # clip value for PPO loss,
        gamma=0.98, # discount factor (empirical formula: 0.1 = gamma^t, where t is the number of future steps that you want your agents to predict {0.96 -> 56 steps, 0.98 - 114 steps, 0.99 -> 229 steps, 0.995 -> 459 steps})
        lmbda=0.9, # lambda for generalised advantage estimation,
        entropy_eps=4e-4, # coefficient of the entropy term in the PPO loss,
        max_steps=2**8, # Episode steps before done (512)
        
        is_save_intermidiate_model=True, # Is this is true, the model with the hightest mean episode reward will be saved,

        episode_reward_mean_current=0.00,
        
        is_load_model=False, # Load offline model if available. The offline model in `where_to_save` whose name contains `episode_reward_mean_current` will be loaded
        is_continue_train=False, # If offline models are loaded, whether to continue to train the model
        mode_name=None, 
        episode_reward_intermidiate=-1e3, # The initial value should be samll enough
        where_to_save=f"outputs/{scenario_name}_ppo/test_nondynamic_goal_reward_no_v_rew_small_goal_threshold/", # folder where to save the trained models, fig, data, etc.
        
        # Scenario parameters
        is_local_observation=False, 
        is_global_coordinate_sys=False,
        n_points_short_term=6,
        is_use_intermediate_goals=False,
        n_nearing_agents_observed=4,
        n_nearing_obstacles_observed=4,
        is_observe_corners=False,
        
        is_testing_mode=False,
        is_visualize_short_term_path=True,
        is_real_time_rendering=True,
        
        path_tracking_type="line", # [relevant to path-tracking scenarios] should be one of 'line', 'turning', 'circle', 'sine', and 'horizontal_8'
        is_dynamic_goal_reward=False, # [relevant to path-tracking scenarios] set to True if the goal reward is dynamically adjusted based on the performance of agents' history trajectories 

        obstacle_type="static", # [relevant for obstacle-avoidance scenarios] should be one of "static" and "dynamic"
    )
    
    scenario = ScenarioObstacleAvoidance()
    scenario.parameters = parameters
    
    render_interactively(
        scenario=scenario, control_two_agents=False, shared_reward=False,
    )
