import os
import sys
import time
# !Important: Add project root to system path if you want to run this file directly
script_dir = os.path.dirname(__file__) # Directory of the current script
project_root = os.path.dirname(script_dir) # Project root directory
if project_root not in sys.path:
    sys.path.append(project_root)
    
import torch

from vmas import render_interactively
from vmas.simulator.core import Agent, Box, World, Line
# from vmas.simulator.dynamics.kinematic_bicycle import KinematicBicycle
from vmas.simulator.scenario import BaseScenario
# from vmas.simulator.utils import Color

from torch import Tensor
from typing import Dict

import matplotlib.pyplot as plt

from utilities.helper_scenario import Distances, Evaluation, Normalizers, Observations, Penalties, ReferencePathsMapRelated, Rewards, Thresholds, Timer, exponential_decreasing_fcn, get_distances_between_agents, get_perpendicular_distances, get_rectangle_corners, get_ref_path_for_tracking_scenarios, get_short_term_reference_path, interX, get_point_line_distance

from utilities.helper_training import Parameters

from utilities.kinematic_bicycle import KinematicBicycle

# Get road data
from utilities.get_cpm_lab_map import get_center_length_yaw_polyline

from utilities.colors import Color
 
# Sample time
dt = 0.1 # [s]

# Geometry
agent_width = 0.10              # The width of the agent in [m]
agent_length = 0.20             # The length of the agent in [m]
agent_wheelbase_front = 0.10    # Front wheelbase in [m]
agent_wheelbase_rear = agent_length - agent_wheelbase_front # Rear wheelbase in [m]

# Maximum control commands
agent_max_speed = 0.5          # Maximum speed in [m/s]
agent_max_steering_angle = 35   # Maximum steering angle in degree

n_agents = 1        # The number of agents
agent_mass = 1    # The mass of each agent in [m]

# Reward
reward_progress = 0.0
reward_speed = 0.1 # ! Should not be a dominant reward, which may encourage agents to move in a high speed but not the right path or direction
reward_reach_goal = 50 # Reward for reaching goal
reward_reach_intermediate_goals = 2 # Reward for reaching intermediate goals

# Penalty for deviating from reference path
penalty_deviate_from_ref_path = -0.0
penalty_deviate_from_goal = -1
# Penalty for leaving the world boundary
penalty_leave_world = -50
# Penalty for changing steering direction
penalty_change_steering = -0.1
# Penalty for losing time
penalty_time = -0.0

# Reference path
n_points_short_term = 12 # The number of points on the short-term reference path

# Observation
is_global_coordinate_sys = True # Set to True if you want to use global coordinate system
is_mixed_scenario_training = True

is_partial_observation = False # TODO Set to True if sensor range is limited
is_add_noise = True
noise_level = 0.2 * agent_width # Noise will be generated by the standary normal distribution. This parameter controls the noise level
n_stored_steps = 5 # The number of steps to store (include the current step). At least one.
n_observed_steps = 1 # The number of steps to observe (include the current step). At least one. At most `n_stored_steps`

is_testing_mode = False # In testing mode, collisions do not lead to the termination of the simulation 
is_visualize_short_term_path = True

threshold_reach_goal = agent_width / 2 # Agents are considered at their goal positions if their distances to the goal positions are less than this threshold
threshold_reach_intermediate_goal = agent_width / 2 # Agents are considered at their intermediate goal positions if their distances to the goal positions are less than this threshold
threshold_change_steering = 10 # [degree]

max_steps = 2000
is_dynamic_goal_reward = False

max_ref_path_points = 120 # Hard coded

class ScenarioPathTracking(BaseScenario):
    def make_world(self, batch_dim: int, device: torch.device, **kwargs):
        self.path_types = ['circle', 'line', 'sine', 'turning'] # Sorted alphabetically
        self.path_ids = torch.zeros(batch_dim, device=device, dtype=torch.int)
        self.batch_indices = torch.arange(batch_dim, device=device)
        
        n_agents_received = kwargs.get("n_agents", n_agents)
        if n_agents_received != 1:
            print(f"In path-tracking scenarios, (only) one agent is needed (but received '{n_agents_received})'.")
        
        self.n_agents = 1 # Only one agent is needed in the path-tracking scenarios
            
        width = kwargs.get("width", agent_width) # Agent width
        l_f = kwargs.get("l_f", agent_wheelbase_front) # Distance between the front axle and the center of gravity
        l_r = kwargs.get("l_r", agent_wheelbase_rear) # Distance between the rear axle and the center of gravity
        max_steering_angle = kwargs.get("max_steering_angle", torch.tensor(agent_max_steering_angle, device=device, dtype=torch.float32).deg2rad())
        max_speed = kwargs.get("max_speed", torch.tensor(agent_max_speed))
        
        # Specify parameters if not given
        if not hasattr(self, "parameters"):
            self.parameters = Parameters(
                is_testing_mode=is_testing_mode,
                is_visualize_short_term_path=is_visualize_short_term_path,
                max_steps=max_steps,
                is_dynamic_goal_reward=is_dynamic_goal_reward,
                path_tracking_type='line',
                is_global_coordinate_sys=is_global_coordinate_sys,
                is_mixed_scenario_training=is_mixed_scenario_training,
            )

        self.timer = Timer( # Timer for the first env
            start=time.time(),
            end=0,
            step=torch.zeros(batch_dim, device=device, dtype=torch.int),
            step_duration=torch.zeros(self.parameters.max_steps, device=device, dtype=torch.float32),
            step_begin=time.time(),
            render_begin=0,
        )
        
        # Hard coded
        world_x_dim = torch.tensor(4.0, device=device, dtype=torch.float32)
        world_y_dim = torch.tensor(4.0, device=device, dtype=torch.float32)

            
        # Initialize reference paths
        self.ref_paths = ReferencePathsMapRelated(
            long_term_agents=torch.zeros((batch_dim, max_ref_path_points, 2), device=device, dtype=torch.float32), # Long-term reference paths of agents
            long_term_vecs_normalized=torch.zeros((batch_dim, max_ref_path_points - 1, 2), device=device, dtype=torch.float32),
            n_points_long_term=torch.zeros(batch_dim, device=device, dtype=torch.int),
            is_loop=torch.zeros(batch_dim, device=device, dtype=torch.bool),
            point_extended=torch.zeros((batch_dim, 2), device=device, dtype=torch.float32),
            n_points_short_term=torch.tensor(n_points_short_term, device=device, dtype=torch.int),
            short_term=torch.zeros((batch_dim, n_points_short_term, 2), device=device, dtype=torch.float32), # Short-term reference path
            short_term_indices = torch.zeros((batch_dim, n_points_short_term), device=device, dtype=torch.int),
        )

        self.start_pos = torch.zeros((batch_dim, 2), device=device, dtype=torch.float32)
        self.start_rot = torch.zeros((batch_dim, 1), device=device, dtype=torch.float32)
        self.start_vel = torch.zeros((batch_dim, 2), device=device, dtype=torch.float32)
        self.goal_pos = torch.zeros((batch_dim, 2), device=device, dtype=torch.float32)
        self.goal_rot = torch.zeros((batch_dim, 1), device=device, dtype=torch.float32)

        self.is_currently_at_goal = torch.zeros(batch_dim, device=device, dtype=torch.bool) # If agents currently are at their goal positions
        self.has_reached_goal  = torch.zeros(batch_dim, device=device, dtype=torch.bool) # Record goal-reaching status
        self.is_leave_world  = torch.zeros(batch_dim, device=device, dtype=torch.bool) # Record if the agent leaves the world
        
        self.normalizers = Normalizers(
            pos=torch.tensor([world_x_dim, world_y_dim], device=device, dtype=torch.float32),
            v=max_speed,
            rot=torch.tensor(2 * torch.pi, device=device, dtype=torch.float32),
            steering=torch.tensor(max_steering_angle, device=device, dtype=torch.float32),
        )
        
        weighting_ref_directions = torch.linspace(1, 0.2, steps=self.ref_paths.n_points_short_term-1, device=device, dtype=torch.float32)
        weighting_ref_directions /= weighting_ref_directions.sum()
        self.rewards = Rewards(
            progress=torch.tensor(reward_progress, device=device, dtype=torch.float32),
            weighting_ref_directions=weighting_ref_directions, # Progress in the weighted directions (directions indicating by closer short-term reference points have higher weights)
            higth_v=torch.tensor(reward_speed, device=device, dtype=torch.float32),
            reach_goal=torch.tensor(reward_reach_goal, device=device, dtype=torch.float32),
            reach_intermediate_goal=torch.tensor(reward_reach_intermediate_goals, device=device, dtype=torch.float32),
        )

        self.penalties = Penalties(
            deviate_from_ref_path=torch.tensor(penalty_deviate_from_ref_path, device=device, dtype=torch.float32),
            deviate_from_goal=torch.tensor(penalty_deviate_from_goal, device=device, dtype=torch.float32),
            weighting_deviate_from_ref_path=torch.tensor(agent_width, device=device, dtype=torch.float32),
            leave_world=torch.tensor(penalty_leave_world, device=device, dtype=torch.float32),
            time=torch.tensor(penalty_time, device=device, dtype=torch.float32),
            change_steering=torch.tensor(penalty_change_steering, device=device, dtype=torch.float32),
        )
        
        self.observations = Observations(
            is_partial=torch.tensor(is_partial_observation, device=device, dtype=torch.bool),
            is_global_coordinate_sys=torch.tensor(self.parameters.is_global_coordinate_sys, device=device, dtype=torch.bool),
            is_add_noise=torch.tensor(is_add_noise, device=device, dtype=torch.bool),
            noise_level=torch.tensor(noise_level, device=device, dtype=torch.float32),
            n_stored_steps=torch.tensor(n_stored_steps, device=device, dtype=torch.int),
            n_observed_steps=torch.tensor(n_observed_steps, device=device, dtype=torch.int),
        )
        assert self.observations.n_stored_steps >= 1, "The number of stored steps should be at least 1."
        assert self.observations.n_observed_steps >= 1, "The number of observed steps should be at least 1."
        assert self.observations.n_stored_steps >= self.observations.n_observed_steps, "The number of stored steps should be greater or equal than the number of observed steps."
        
        self.observations.past_pos = torch.zeros((batch_dim, self.observations.n_stored_steps, 2), device=device, dtype=torch.float32) # Past + current
        self.observations.past_vel = torch.zeros((batch_dim, self.observations.n_stored_steps, 2), device=device, dtype=torch.float32) # Past + current
        self.observations.past_action_vel = torch.zeros((batch_dim, self.observations.n_stored_steps), device=device, dtype=torch.float32)
        self.observations.past_action_steering = torch.zeros((batch_dim, self.observations.n_stored_steps), device=device, dtype=torch.float32)
        self.observations.past_distance_to_ref_path = torch.zeros((batch_dim, self.observations.n_stored_steps), device=device, dtype=torch.float32)
        
        # Distances to boundaries and reference path, and also the closest point on the reference paths of agents
        self.distances = Distances(
            ref_paths=torch.zeros((batch_dim, self.n_agents), device=device, dtype=torch.float32),
            closest_point_on_ref_path=torch.zeros((batch_dim, self.n_agents), device=device, dtype=torch.int),
            goal=torch.zeros((batch_dim, self.n_agents), device=device, dtype=torch.float32),
        )
        
        self.thresholds = Thresholds(
            reach_goal=torch.tensor(threshold_reach_goal, device=device, dtype=torch.float32),
            reach_intermediate_goal=torch.tensor(threshold_reach_intermediate_goal, device=device, dtype=torch.float32),
            change_steering=torch.tensor(threshold_change_steering, device=device, dtype=torch.float32).deg2rad(),
        )
        
        if self.parameters.is_dynamic_goal_reward:
            self.evaluation = Evaluation(
                pos_traj=torch.zeros((batch_dim, self.n_agents, self.parameters.max_steps, 2), device=device, dtype=torch.float32),
                v_traj=torch.zeros((batch_dim, self.n_agents, self.parameters.max_steps, 2), device=device, dtype=torch.float32),
                rot_traj=torch.zeros((batch_dim, self.n_agents, self.parameters.max_steps), device=device, dtype=torch.float32),
                deviation_from_ref_path=torch.zeros((batch_dim, self.n_agents, self.parameters.max_steps), device=device, dtype=torch.float32),
                path_tracking_error_mean=torch.zeros((batch_dim, self.n_agents), device=device, dtype=torch.float32),
            )

        # Store the reaching status of intermediate goals
        self.are_intermediate_goals_reached = torch.zeros((batch_dim, self.n_agents, self.ref_paths.long_term_agents.shape[1]), device=device, dtype=torch.bool)
        
        # Make world
        world = World(batch_dim, device, x_semidim=world_x_dim, y_semidim=world_y_dim, dt=dt)
        # world._drag = 0 # !No drag/friction

        # Use the kinematic bicycle model for the agent
        agent = Agent(
            name=f"agent_0",
            shape=Box(length=l_f+l_r, width=width),
            collide=False,
            render_action=False,
            u_range=[max_speed, max_steering_angle], # Control command serves as velocity command 
            u_multiplier=[1, 1],
            max_speed=max_speed,
            dynamics=KinematicBicycle(
                world, 
                width=width, 
                l_f=l_f, 
                l_r=l_r, 
                max_steering_angle=max_steering_angle, 
                integration="rk4" # one of "euler", "rk4"
            )
        )
        # Create a variable to store the position of the agent at the previous time step 
        agent.state.pos_previous = torch.zeros((batch_dim, 2), device=device, dtype=torch.float32)
        world.add_agent(agent)

        return world

    def reset_world_at(self, env_index: int = None):
        # print(f"[DEBUG] reset_world_at(): env_index = {env_index}")
        """
        This function resets the world at the specified env_index.

        Args:
        :param env_index: index of the environment to reset. If None a vectorized reset should be performed

        """
        agent_i = 0 # Only one agent
        agent = self.world.agents[agent_i]
        
        # if hasattr(self, 'training_info'):
        #     print(self.training_info["agents"]["episode_reward"].mean())
        
        if (env_index is None) or (env_index == 0):
            self.timer.step_duration[:] = 0
            self.timer.start = time.time()
            self.timer.step_begin = time.time()
            self.timer.end = 0

        # Reset variables for the agent
        if env_index is None: # Reset all envs
            
            if self.parameters.is_mixed_scenario_training:
                # Randomly choose from one of the paths when resetting the world
                self.path_ids[:] = torch.randint(len(self.path_types), (self.world.batch_dim,))
            else:
                self.path_ids[:] = self.path_types.index(self.parameters.path_tracking_type)
                
            for env_i in range(self.world.batch_dim):
                tracking_path, num_points, ranges, self.start_pos[env_i, :], self.start_rot[env_i, :], self.start_vel[env_i, :], self.goal_pos[env_i, :], self.goal_rot[env_i, :], is_ref_path_loop, point_extended = get_ref_path_for_tracking_scenarios(
                    path_tracking_type=self.path_types[self.path_ids[env_i]], 
                    agent_width=agent.shape.width,
                    agent_length=agent.shape.length,
                    point_interval=agent.shape.width,
                    max_speed=agent.max_speed,
                    device=self.world.device,
                    center_point=torch.tensor([0, 0], device=self.world.device, dtype=torch.float32), 
                    is_visualize=False,
                    is_save_fig=False,
                    max_ref_path_points = max_ref_path_points,
                )
                
                self.ref_paths.long_term_agents[env_i, :] = tracking_path.clone()
                _, lengths_path, _, vecs_path = get_center_length_yaw_polyline(self.ref_paths.long_term_agents[env_i, :])
                self.ref_paths.long_term_vecs_normalized[env_i, :] = vecs_path / lengths_path.unsqueeze(1)
                self.ref_paths.long_term_vecs_normalized[env_i, num_points-1:] = self.ref_paths.long_term_vecs_normalized[env_i, num_points-2] # Remove nan
                self.ref_paths.n_points_long_term[env_i] = num_points
                self.ref_paths.is_loop[env_i] = is_ref_path_loop
                self.ref_paths.point_extended[env_i] = point_extended

                agent.set_pos(self.start_pos[env_i, :], batch_index=env_i)
                agent.set_rot(self.start_rot[env_i, :], batch_index=env_i)
                agent.set_vel(self.start_vel[env_i, :], batch_index=env_i)

        
            # Reset distances to the reference path          
            self.distances.ref_paths[:], self.distances.closest_point_on_ref_path[:] = get_perpendicular_distances(
                point=agent.state.pos, 
                polyline=self.ref_paths.long_term_agents,
                n_points_long_term=self.ref_paths.n_points_long_term,
            )

            self.distances.goal[:, agent_i] = (agent.state.pos - self.goal_pos).norm(dim=1)

            self.ref_paths.short_term[:], self.ref_paths.short_term_indices[:] = get_short_term_reference_path( 
                reference_path=self.ref_paths.long_term_agents,
                closest_point_on_ref_path=self.distances.closest_point_on_ref_path,
                n_points_short_term=self.ref_paths.n_points_short_term, 
                device=self.world.device,
                is_ref_path_loop=self.ref_paths.is_loop,
                point_extended=self.ref_paths.point_extended,
                n_points_long_term=self.ref_paths.n_points_long_term,
            )
            
            self.is_currently_at_goal[:] = False
            self.has_reached_goal[:] = False
            self.is_leave_world[:] = False
            
            if self.parameters.is_dynamic_goal_reward:
                # Reset the data for evaluation for all envs
                self.evaluation.pos_traj[:] = 0
                self.evaluation.v_traj[:] = 0
                self.evaluation.rot_traj[:] = 0
                self.evaluation.path_tracking_error_mean[:] = 0

            # Reset the previous position
            agent.state.pos_previous = agent.state.pos.clone()
            
            # Reset the reaching status of intermediate goals for all envs
            self.are_intermediate_goals_reached[:] = False
            
            self.observations.past_pos[:] = 0
            self.observations.past_vel[:] = 0
            self.observations.past_action_vel[:] = 0
            self.observations.past_action_steering[:] = 0
            self.observations.past_distance_to_ref_path[:] = 0
            
            self.timer.step[:] = 0

        else: # Reset the env specified by `env_index` 
            if self.parameters.is_mixed_scenario_training:
                # Randomly choose from one of the paths when resetting the world
                self.path_ids[env_index] = torch.randint(len(self.path_types), (1,))

            tracking_path, num_points, ranges, self.start_pos[env_index, :], self.start_rot[env_index, :], self.start_vel[env_index, :], self.goal_pos[env_index, :], self.goal_rot[env_index, :], is_ref_path_loop, point_extended = get_ref_path_for_tracking_scenarios(
                path_tracking_type=self.path_types[self.path_ids[env_index]], 
                agent_width=agent.shape.width,
                agent_length=agent.shape.length,
                point_interval=agent.shape.width,
                max_speed=agent.max_speed,
                device=self.world.device,
                center_point=torch.tensor([0, 0], device=self.world.device, dtype=torch.float32), 
                is_visualize=False,
                is_save_fig=False,
                max_ref_path_points = max_ref_path_points,
            )
            
            self.ref_paths.long_term_agents[env_index, :] = tracking_path.clone()
            _, lengths_path, _, vecs_path = get_center_length_yaw_polyline(self.ref_paths.long_term_agents[env_index, :])
            self.ref_paths.long_term_vecs_normalized[env_index, :] = vecs_path / lengths_path.unsqueeze(1)
            self.ref_paths.long_term_vecs_normalized[env_index, num_points-1:] = self.ref_paths.long_term_vecs_normalized[env_index, num_points-2] # Remove nan
            self.ref_paths.n_points_long_term[env_index] = num_points
            self.ref_paths.is_loop[env_index] = is_ref_path_loop
            self.ref_paths.point_extended[env_index] = point_extended

            agent.set_pos(self.start_pos[env_index, :], batch_index=env_index)
            agent.set_rot(self.start_rot[env_index, :], batch_index=env_index)
            agent.set_vel(self.start_vel[env_index, :], batch_index=env_index)
                
            self.is_currently_at_goal[env_index] = False
            self.has_reached_goal[env_index] = False
            self.is_leave_world[env_index] = False
            
            # Reset distances to the reference path          
            self.distances.ref_paths[env_index], self.distances.closest_point_on_ref_path[env_index] = get_perpendicular_distances(
                point=agent.state.pos[env_index,:].unsqueeze(0), 
                polyline=self.ref_paths.long_term_agents[env_index],
                n_points_long_term=self.ref_paths.n_points_long_term[env_index],
            )
            
            self.distances.goal[env_index, agent_i] = (agent.state.pos[env_index,:] - self.goal_pos[env_index,:]).norm(dim=-1)

            # Reset the short-term reference path of agents in env `env_index`
            self.ref_paths.short_term[env_index], self.ref_paths.short_term_indices[env_index] = get_short_term_reference_path( 
                reference_path=self.ref_paths.long_term_agents[env_index].unsqueeze(0),
                closest_point_on_ref_path=self.distances.closest_point_on_ref_path[env_index].unsqueeze(0),
                n_points_short_term=self.ref_paths.n_points_short_term, 
                device=self.world.device,
                is_ref_path_loop=self.ref_paths.is_loop[env_index].unsqueeze(0),
                point_extended=self.ref_paths.point_extended[env_index].unsqueeze(0),
                n_points_long_term=self.ref_paths.n_points_long_term[env_index].unsqueeze(0),
            )
            
            if self.parameters.is_dynamic_goal_reward:
                # Reset the data for evaluation for env `env_index`
                self.evaluation.pos_traj[env_index] = 0
                self.evaluation.v_traj[env_index] = 0
                self.evaluation.rot_traj[env_index] = 0
                self.evaluation.path_tracking_error_mean[env_index] = 0
            
            # Reset the previous position for env `env_index`
            agent.state.pos_previous[env_index,:] = agent.state.pos[env_index,:].clone()
        
            # Reset the reaching status of intermediate goals for env `env_index`
            self.are_intermediate_goals_reached[env_index, :] = False
            
            self.observations.past_pos[env_index, :] = 0
            self.observations.past_vel[env_index, :] = 0
            self.observations.past_action_vel[env_index, :] = 0
            self.observations.past_action_steering[env_index, :] = 0
            self.observations.past_distance_to_ref_path[env_index, :] = 0
        
            self.timer.step[env_index] = 0


    def process_action(self, agent: Agent):
        """This function will be executed before the step function, i.e., states are not updated yet."""
        # print("[DEBUG] process_action()")
        if hasattr(agent, 'dynamics') and hasattr(agent.dynamics, 'process_force'):
            agent.dynamics.process_force()
        else:
            # The agent does not have a dynamics property, or it does not have a process_force method
            pass

    def reward(self, agent: Agent):
        # print("[DEBUG] reward()")
        # Initialize
        self.rew = torch.zeros(self.world.batch_dim, device=self.world.device, dtype=torch.float32)
        agent_index = self.world.agents.index(agent) # Get the index of the current agent
        
        # Timer
        if agent_index == 0:
            self.timer.step_duration[self.timer.step] = time.time() - self.timer.step_begin                
            self.timer.step_begin = time.time() # Set to the current time as the begin of the current time step
            # Increment step by 1
            self.timer.step += 1
            
        # Calculate the distance from the center of the agent to its reference path
        self.distances.ref_paths[:], self.distances.closest_point_on_ref_path[:] = get_perpendicular_distances(
            point=agent.state.pos, 
            polyline=self.ref_paths.long_term_agents,
            n_points_long_term=self.ref_paths.n_points_long_term,
        )

        if self.parameters.is_dynamic_goal_reward:
            # Update data for evaluation
            self.evaluation.pos_traj[:,agent_index,self.timer.step] = agent.state.pos
            self.evaluation.v_traj[:,agent_index,self.timer.step] = agent.state.vel
            self.evaluation.rot_traj[:,agent_index,self.timer.step] = agent.state.rot.squeeze(1)
            self.evaluation.deviation_from_ref_path[:,agent_index,self.timer.step] = self.distances.ref_paths[:,agent_index]
            self.evaluation.path_tracking_error_mean[:,agent_index] = self.evaluation.deviation_from_ref_path[:,agent_index].sum(dim=1) / (self.timer.step + 1) # Step starts from 0

        ##################################################
        ## Penalty for deviating from reference path
        ##################################################
        self.rew += self.distances.ref_paths[:,agent_index] / self.penalties.weighting_deviate_from_ref_path * self.penalties.deviate_from_ref_path       
        
        ###################################################
        ## Reward for the actual movement
        ##################################################
        # TODO Check if this is necessary when there are rewards for intermediate goals
        movement = (agent.state.pos - agent.state.pos_previous) # Calculate the progress of the agent
        # Handle non-loop reference path
        len_long_term_ref_path = len(self.ref_paths.long_term_agents[agent_index])
        short_term_indices = torch.where(self.ref_paths.short_term_indices == len_long_term_ref_path - 1, len_long_term_ref_path - 2, self.ref_paths.short_term_indices)
        short_term_path_vec_normalized = self.ref_paths.long_term_vecs_normalized[
            self.batch_indices.unsqueeze(1), 
            short_term_indices[:, 0:-1]
        ] # Narmalized vector of the short-term reference path
        movement_normalized_proj = torch.sum(movement.unsqueeze(1) * short_term_path_vec_normalized, dim=2)
        movement_normalized_proj = torch.where(movement_normalized_proj < 0, movement_normalized_proj * 2, movement_normalized_proj) # Penalize more if move backwards
        movement_weighted_sum_proj = torch.matmul(movement_normalized_proj, self.rewards.weighting_ref_directions)
        self.rew += movement_weighted_sum_proj / (agent.max_speed * self.world.dt) * self.rewards.progress # Relative to the maximum possible movement
        
        ##################################################
        ## Reward for intermediate goals
        ##################################################
        # All points on the reference path are considered "intermediate goals" 
        distances_to_intermediate_goals, is_projection_inside_line = get_point_line_distance(
            points=self.ref_paths.long_term_agents[agent_index],
            lines_start_points=agent.state.pos_previous,
            lines_end_points=agent.state.pos # Use not only the current position but also the previous position to cover the case that the an intermediate goal lies between the previous and the current positions
        )
        # Update the reaching status of intermediate goals (a intermediate goal is only considered reached if its projection is inside the agent's trajectry)
        are_intermediate_goals_reached = (distances_to_intermediate_goals <= self.thresholds.reach_intermediate_goal) & is_projection_inside_line
        # Create the mask where each element is True if it's index is greater than the corresponding value in indices
        row_range = torch.arange(max_ref_path_points, device=self.world.device).unsqueeze(0).expand_as(are_intermediate_goals_reached)
        mask = row_range >= self.ref_paths.n_points_long_term.unsqueeze(1)
        are_intermediate_goals_reached[mask] = False # Repeated points appended at the end do not count 
        
        new_intermediate_goals_reached = (~self.are_intermediate_goals_reached[:, agent_index, :]) & are_intermediate_goals_reached
        # Update intermediate-goal-reaching status
        self.are_intermediate_goals_reached[:, agent_index, :] |= new_intermediate_goals_reached
        factor_intermediate_goals_reached = torch.zeros((self.world.batch_dim), device=self.world.device, dtype=torch.float32)
        for idx, dist in enumerate(distances_to_intermediate_goals):
            factor_intermediate_goals_reached[idx] = (self.thresholds.reach_intermediate_goal - (dist[new_intermediate_goals_reached[idx, :]]).mean()) / self.thresholds.reach_intermediate_goal
            factor_intermediate_goals_reached[factor_intermediate_goals_reached.isnan()] = 0
                    
        self.rew += new_intermediate_goals_reached.sum(dim=1) * self.rewards.reach_intermediate_goal * factor_intermediate_goals_reached

        ##################################################
        ## Reward for moving in a high velocity and the right direction
        ##################################################
        # TODO: optional?
        v_proj = torch.sum(agent.state.vel.unsqueeze(1) * short_term_path_vec_normalized, dim=2)
        v_proj_weighted_sum = torch.matmul(v_proj, self.rewards.weighting_ref_directions)
        self.rew += v_proj_weighted_sum / agent.max_speed * self.rewards.higth_v

        # Save previous positions
        agent.state.pos_previous = agent.state.pos.clone() 


        ##################################################
        ## Reward for reaching goal (only when reference path is not a loop)
        ##################################################
        # Update distances to goal positions
        self.distances.goal[:, agent_index] = distances_to_intermediate_goals[self.batch_indices, self.ref_paths.n_points_long_term - 1]
        is_currently_at_goal = are_intermediate_goals_reached[self.batch_indices, self.ref_paths.n_points_long_term - 1] * (~self.ref_paths.is_loop)
        goal_reward_factor = (self.thresholds.reach_goal - self.distances.goal[:, agent_index]) / self.thresholds.reach_goal
        self.rew += is_currently_at_goal * ~self.has_reached_goal * self.rewards.reach_goal * goal_reward_factor # Agents can only receive the goal reward once per iteration
        # Update goal-reaching status
        self.has_reached_goal[is_currently_at_goal] = True

        ##################################################
        ## Penalty for leaving goal position (not relevant to single-agent scenario)
        ##################################################
        # This penalty is only applied to agents that have reached their goal positions before and now leave them, aiming to encourage these agents to stay at their goal positions
        self.rew += (self.distances.goal[:, agent_index] - self.thresholds.reach_goal).clamp(min=0) * self.penalties.deviate_from_goal * self.has_reached_goal 
        # Reset the intermediate goals if the reference path is a loop
        is_reset_intermediate_goals = (self.are_intermediate_goals_reached.sum(dim=(1,2)) > (0.75 * self.ref_paths.n_points_long_term)) & self.ref_paths.is_loop # Resear only when a large amount of intermediate goals are reached
        self.are_intermediate_goals_reached[is_reset_intermediate_goals,:] = False

        ##################################################
        ## Penalty for leaving the world
        ##################################################
        is_leave_world = (
            (agent.state.pos[:, 0] < -self.world.x_semidim / 2) | 
            (agent.state.pos[:, 0] > self.world.x_semidim / 2) | 
            (agent.state.pos[:, 1] < -self.world.y_semidim / 2) | 
            (agent.state.pos[:, 1] > self.world.y_semidim / 2)
        )
        self.is_leave_world[is_leave_world] = True
        self.rew += self.is_leave_world * self.penalties.leave_world
        
        ##################################################
        ## Penalty for changing steering angle too much
        ##################################################
        steering_change = torch.clamp((self.observations.past_action_steering[:, -1] - self.observations.past_action_steering[:, -2]).abs() - self.thresholds.change_steering, min=0)
        steering_change_reward_factor = steering_change / (2 * agent.u_range[1] - self.thresholds.change_steering)
        self.rew += steering_change_reward_factor * self.penalties.change_steering
        # print(f"Penality for changing steering angle {steering_change_reward_factor * self.penalties.change_steering}")
        
        
        ##################################################
        ## Penalty for losing time
        ##################################################
        self.rew += self.penalties.time

        # Update the short-term reference path (extract from the long-term reference path)
        self.ref_paths.short_term[:], self.ref_paths.short_term_indices[:] = get_short_term_reference_path(
            reference_path=self.ref_paths.long_term_agents, 
            closest_point_on_ref_path=self.distances.closest_point_on_ref_path,
            n_points_short_term=self.ref_paths.n_points_short_term, 
            device=self.world.device,
            is_ref_path_loop=self.ref_paths.is_loop,
            point_extended=self.ref_paths.point_extended,
            n_points_long_term=self.ref_paths.n_points_long_term,
        )
                
        return self.rew


    def observation(self, agent: Agent):
        # print("[DEBUG] observation()")
        """
        Generate an observation for the given agent in all envs.

        Parameters:
        - agent (Agent): The agent for which the observation is to be generated.

        Returns:
        - The observation for the given agent in all envs.
        """

        if self.observations.is_add_noise:
            # Generate random noise from a standard normal distribution N(0, 1). Multiply a factor to adjust the noice level
            noise_ref_path = torch.randn(self.world.batch_dim, self.ref_paths.n_points_short_term, 2) * self.observations.noise_level #  torch.randn_like generates ramdom noise with the same shape as the input
            noise_distances_to_ref_path = torch.randn(self.world.batch_dim) * self.observations.noise_level
        else:
            noise_ref_path = 0
            noise_distances_to_ref_path = 0

        if self.observations.is_global_coordinate_sys:
            positions = agent.state.pos / self.normalizers.pos
            velocities = agent.state.vel / self.normalizers.v
            ##################################################
            ## Observation of short-term reference path
            ##################################################
            # Skip the first point on the short-term reference path, because, mostly, it is behind the agent. The second point is in front of the agent.
            obs_ref_point_norm = ((self.ref_paths.short_term + noise_ref_path) / self.normalizers.pos).reshape(self.world.batch_dim, -1)
        else:
            positions = torch.zeros((self.world.batch_dim, 2)) # Positions are at the origin of the local coordinate system
            velocities = agent.state.vel / self.normalizers.v
            velocities[:,0] = velocities.norm(dim=-1)
            velocities[:,1] = 0
            
            ##################################################
            ## Observation of short-term reference path
            ##################################################
            # Normalized short-term reference path relative to the agent's current position
            # Skip the first point on the short-term reference path, because, mostly, it is behind the agent. The second point is in front of the agent.
            ref_points_rel_norm = ((self.ref_paths.short_term + noise_ref_path) - agent.state.pos.unsqueeze(1)) / self.normalizers.pos
            ref_points_rel_abs = ref_points_rel_norm.norm(dim=2)
            ref_points_rel_rot = torch.atan2(ref_points_rel_norm[:,:,1], ref_points_rel_norm[:,:,0]) - agent.state.rot
            obs_ref_point_norm = torch.stack(
                (
                    ref_points_rel_abs * torch.cos(ref_points_rel_rot), 
                    ref_points_rel_abs * torch.sin(ref_points_rel_rot)    
                ), dim=2
            ).reshape(self.world.batch_dim, -1)

        empty_actions = torch.zeros( # Will be used in case that actions are None at the begining
            (self.world.batch_dim, agent.action.action_size), device=self.world.device, dtype=torch.float32
        )
        
        ##################################################
        ## Observation of positions
        ##################################################
        # Shift old observations by one step and add the current observations
        self.observations.past_pos[:, 0:-1] = self.observations.past_pos[:, 1:].clone()
        self.observations.past_pos[:, -1] = positions.clone()
        
        ##################################################
        ## Observation of velocities
        ##################################################
        # Shift old observations by one step and add the current observations
        self.observations.past_vel[:, 0:-1] = self.observations.past_vel[:, 1:].clone()
        self.observations.past_vel[:, -1] = velocities.clone()
        
        ##################################################
        ## Observation of actions
        ##################################################
        self.observations.past_action_vel[:, 0:-1] = self.observations.past_action_vel[:, 1:].clone()
        self.observations.past_action_vel[:, -1] = agent.action.u[:, 0].clone() if (agent.action.u is not None) else empty_actions[:, 0]
        
        self.observations.past_action_steering[:, 0:-1] = self.observations.past_action_steering[:, 1:].clone()
        self.observations.past_action_steering[:, -1] = agent.action.u[:, 1].clone() if (agent.action.u is not None) else empty_actions[:, 0]
        
        ##################################################
        ## Observation of distance to reference path
        ##################################################
        self.observations.past_distance_to_ref_path[:, 0:-1] = self.observations.past_distance_to_ref_path[:, 1:].clone()
        self.observations.past_distance_to_ref_path[:, -1] = self.distances.ref_paths.squeeze(1) + noise_distances_to_ref_path

        index_start = self.observations.n_stored_steps-self.observations.n_observed_steps
        
        obs_list = [
                (self.observations.past_vel[:, index_start:] / self.normalizers.v).reshape(self.world.batch_dim, -1),
                self.observations.past_action_vel[:, index_start:] / self.normalizers.v,
                self.observations.past_action_steering[:, index_start:] / self.normalizers.steering,
                self.observations.past_distance_to_ref_path[:, index_start:] / self.normalizers.pos.norm(),
                obs_ref_point_norm,
        ]
        
        if self.observations.is_global_coordinate_sys:
            obs_list.insert(
                0,
                (self.observations.past_pos[:, index_start:] / self.normalizers.pos).reshape(self.world.batch_dim, -1)
            )
            
        return torch.hstack(obs_list)


    def done(self):
        # Initialize
        is_done = torch.zeros(self.world.batch_dim, device=self.world.device, dtype=torch.bool)
        
        # Reaching final goal terminates an episode
        is_done |= self.has_reached_goal & ~self.ref_paths.is_loop # Scenarios with loop reference path have no final goal

        # Leaving the world boundaries terminates an episode
        is_done |= self.is_leave_world
        
        # Reaching the maximum time steps terminates an episode
        is_max_steps_reached = self.timer.step == (self.parameters.max_steps - 1)
        is_done |= is_max_steps_reached

        # Logs
        if self.has_reached_goal.any():
            print("Reach goal!")
        if self.is_leave_world.any():
            print("Leave the world!")
        if is_max_steps_reached.any():
            print("The number of the maximum steps is reached.")

        if self.parameters.is_testing_mode:
            is_done = torch.zeros(is_done.shape, device=self.world.device, dtype=torch.bool)
            if self.timer.step % 20 == 0:
                print("You are in testing mode. Collisions do not terminate the simulation.")
        
        return is_done

    def info(self, agent: Agent) -> Dict[str, Tensor]:
        """
        This function computes the info dict for 'agent' in a vectorized way
        The returned dict should have a key for each info of interest and the corresponding value should
        be a tensor of shape (n_envs, info_size)

        Implementors can access the world at 'self.world'

        To increase performance, tensors created should have the device set, like:
        torch.tensor(..., device=self.world.device)

        :param agent: Agent batch to compute info of
        :return: info: A dict with a key for each info of interest, and a tensor value  of shape (n_envs, info_size)
        """
        agent_index = self.world.agents.index(agent) # Index of the current agent
        
        info = {
            "pos": agent.state.pos,
            "vel": agent.state.vel,
            "rot": agent.state.rot,
            "deviation_from_ref_path": self.distances.ref_paths[:,agent_index],
        }
        
        return info
    
    def extra_render(self, env_index: int = 0):
        from vmas.simulator import rendering

        if self.parameters.is_real_time_rendering:
            if self.timer.step[0] == 0:
                pause_duration = 0 # Not sure how long should the simulation be paused at time step 0, so rather 0
            else:
                pause_duration = self.world.dt - (time.time() - self.timer.render_begin)
            if pause_duration > 0:
                time.sleep(pause_duration)
            # print(f"Paused for {pause_duration} sec.")
            
            self.timer.render_begin = time.time() # Update


        geoms = []
        
        # Visualize reference path
        geom = rendering.PolyLine(
            v = self.ref_paths.long_term_agents[env_index],
            close=False,
        )
        
        xform = rendering.Transform()
        geom.add_attr(xform)
        geom.set_color(*Color.black100)
        geoms.append(geom)
                
        # Visualize goal
        if not self.ref_paths.is_loop[env_index]:
            color = Color.red100
            circle = rendering.make_circle(radius=self.thresholds.reach_goal, filled=True)
            xform = rendering.Transform()
            circle.add_attr(xform)
            xform.set_translation(self.goal_pos[env_index, 0], self.goal_pos[env_index, 1])
            circle.set_color(*color)
            geoms.append(circle)

        # Visualize short-term reference paths
        if self.parameters.is_visualize_short_term_path:
            geom = rendering.PolyLine(
                v = self.ref_paths.short_term[env_index],
                close=False,
            )
            xform = rendering.Transform()
            geom.add_attr(xform)            
            geom.set_color(*Color.green100)
            geoms.append(geom)
                        
        # Visualize obstacles
        if "obstacle" in self.parameters.scenario_name:
            if self.parameters.obstacle_type == "static":
                geom = rendering.FilledPolygon(
                    v = self.obstacles.corners,
                )
                xform = rendering.Transform()
                geom.add_attr(xform)            
                geom.set_color(*Color.blue50)
                geoms.append(geom)

            elif self.parameters.obstacle_type == "dynamic":
                corners = self.obstacles.corners[:, self.timer.step[env_index]]
                for i_c in corners:                
                    geom = rendering.FilledPolygon(
                        v = i_c,
                    )

                    xform = rendering.Transform()
                    
                    geom.add_attr(xform)            
                    geom.set_color(*Color.blue50)
                    geoms.append(geom)
            
        return geoms



if __name__ == "__main__":
    scenario_name = "car_like_robots_path_tracking" # car_like_robots_road_traffic, car_like_robots_go_to_formation, car_like_robots_path_tracking
    parameters = Parameters(
        n_agents=4,
        dt=0.1, # [s] sample time 
        device="cpu" if not torch.backends.cuda.is_built() else "cuda:0",  # The divice where learning is run
        scenario_name=scenario_name,
        
        # Training parameters
        n_iters=100, # Number of sampling and training iterations (on-policy: rollouts are collected during sampling phase, which will be immediately used in the training phase of the same iteration),
        frames_per_batch=2**10, # Number of team frames collected per training iteration (minibatch_size*10)
        num_epochs=30, # Number of optimization steps per training iteration,
        minibatch_size=2**8, # Size of the mini-batches in each optimization step (2**9 - 2**12?),
        lr=4e-4, # Learning rate,
        max_grad_norm=1.0, # Maximum norm for the gradients,
        clip_epsilon=0.2, # clip value for PPO loss,
        gamma=0.98, # discount factor (empirical formula: 0.1 = gamma^t, where t is the number of future steps that you want your agents to predict {0.96 -> 56 steps, 0.98 - 114 steps, 0.99 -> 229 steps, 0.995 -> 459 steps})
        lmbda=0.9, # lambda for generalised advantage estimation,
        entropy_eps=4e-4, # coefficient of the entropy term in the PPO loss,
        max_steps=2**7, # Episode steps before done (512)
        
        is_save_intermidiate_model=True, # Is this is true, the model with the hightest mean episode reward will be saved,
        n_nearing_agents_observed=4,
        episode_reward_mean_current=0.00,
        
        is_load_model=False, # Load offline model if available. The offline model in `where_to_save` whose name contains `episode_reward_mean_current` will be loaded
        is_continue_train=False, # If offline models are loaded, whether to continue to train the model
        mode_name=None, 
        episode_reward_intermidiate=-1e3, # The initial value should be samll enough
        where_to_save=f"outputs/{scenario_name}_ppo/test_nondynamic_goal_reward_no_v_rew_small_goal_threshold/", # folder where to save the trained models, fig, data, etc.
        is_mixed_scenario_training = True,
        
        # Scenario parameters
        is_partial_observation=False, 
        is_global_coordinate_sys=True,
        n_points_short_term=6,
        
        is_testing_mode=False,
        is_visualize_short_term_path=True,
        is_real_time_rendering=True,
        
        path_tracking_type='line', # [relevant to path-tracking scenarios] should be one of 'line', 'turning', 'circle', 'sine', and 'horizontal_8'
        is_dynamic_goal_reward=False, # [relevant to path-tracking scenarios] set to True if the goal reward is dynamically adjusted based on the performance of agents' history trajectories 
    )
    
    scenario = ScenarioPathTracking()
    scenario.parameters = parameters
    
    render_interactively(
        scenario=scenario, control_two_agents=False, shared_reward=False,
    )
