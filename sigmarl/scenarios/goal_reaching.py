# Copyright (c) 2024, Chair of Embedded Software (Informatik 11) - RWTH Aachen University.
#
# This source code is licensed under the MIT license found in the
# LICENSE file in the root directory of this source tree.

# Add project root to system path
import time
from termcolor import colored, cprint

import torch
from torch import Tensor
import torch.nn.functional as F
from typing import Dict

from vmas.simulator import rendering

# Enable anomaly detection
# torch.autograd.set_detect_anomaly(True)

import matplotlib.pyplot as plt

from vmas import render_interactively
from vmas.simulator.core import Agent, Box, World
from vmas.simulator.scenario import BaseScenario

from sigmarl.dynamics import KinematicBicycleModel

from sigmarl.colors import Color, colors

from sigmarl.helper_training import Parameters, WorldCustom, Vehicle

from sigmarl.helper_scenario import (
    CircularBuffer,
    Distances,
    Normalizers,
    Observations,
    Penalties,
    ReferencePathsAgentRelated,
    ReferencePathsMapRelated,
    Rewards,
    Thresholds,
    Timer,
    Constants,
    StateBuffer,
    exponential_decreasing_fcn,
    get_distances_between_agents,
    get_perpendicular_distances,
    get_rectangle_vertices,
    get_short_term_reference_path,
    interX,
    angle_eliminate_two_pi,
    transform_from_global_to_local_coordinate,
)

from sigmarl.constants import SCENARIOS, AGENTS


class GoalReaching(BaseScenario):
    """
    Provides a simple scenario where an agent tries to reach its goal (destination).
    """

    def make_world(self, batch_dim: int, device: torch.device, **kwargs):
        self._init_params(batch_dim, device, **kwargs)
        world = self._init_world(batch_dim, device)
        self._init_agents(world)
        self.is_obs_steering = False
        return world

    def _init_params(self, batch_dim, device, **kwargs):
        """
        Initialize parameters.
        """
        scenario_type = kwargs.pop(
            "scenario_type", "goal_reaching_1"
        )  # Scenario type. See all scenario types in SCENARIOS in utilities/constants
        self.agent_width = AGENTS["width"]  # The width of the agent in [m]
        self.agent_length = AGENTS["length"]  # The length of the agent in [m]
        lane_width = SCENARIOS[scenario_type][
            "lane_width"
        ]  # The (rough) width of each lane in [m]

        # Reward
        r_p_normalizer = (
            100  # This parameter normalizes rewards and penalties to [-1, 1].
        )
        # This is useful for RL algorithms with an actor-critic architecture where the critic's
        # output is limited to [-1, 1] (e.g., due to tanh activation function).

        reward_progress = (
            kwargs.pop("reward_progress", 10) / r_p_normalizer
        )  # Reward for moving along reference paths
        reward_vel = (
            kwargs.pop("reward_vel", 5) / r_p_normalizer
        )  # Reward for moving in high velocities.
        reward_reach_goal = (
            kwargs.pop("reward_reach_goal", 100) / r_p_normalizer
        )  # Goal-reaching reward

        # Penalty
        penalty_deviate_from_ref_path = kwargs.pop(
            "penalty_deviate_from_ref_path", -2 / r_p_normalizer
        )  # Penalty for deviating from reference paths
        penalty_time = kwargs.pop(
            "penalty_time", 0 / r_p_normalizer
        )  # Penalty for losing time
        penalty_change_steering = kwargs.pop(
            "penalty_change_steering", -2 / r_p_normalizer
        )  # Penalty for changing steering too quick

        penalty_leave_world = (
            kwargs.pop("penalty_leave_world", -100) / r_p_normalizer
        )  # Penalty for leaving the world

        threshold_reach_goal = kwargs.pop(
            "threshold_reach_goal", self.agent_length
        )  # Threshold less than which agents are considered at their goal positions

        threshold_change_steering = kwargs.pop(
            "threshold_change_steering", 10
        )  # Threshold above which agents will be penalized for changing steering too quick [degree]

        # Reference path
        self.distance_between_points_ref_path = kwargs.pop(
            "sample_distance", self.agent_length
        )  # Distance between the points in the short-term reference paths

        noise_level = kwargs.pop(
            "noise_level", 0.1
        )  # Noise will be generated by the standary normal distribution. This parameter controls the noise level

        threshold_deviate_from_ref_path = kwargs.pop(
            "threshold_deviate_from_ref_path", self.agent_width
        )  # Use for penalizing of deviating from reference path

        # World dimensions
        self.world_x_dim = SCENARIOS[scenario_type]["world_x_dim"]
        self.world_y_dim = SCENARIOS[scenario_type]["world_y_dim"]

        # World ranges
        self.x_dim_min = SCENARIOS[scenario_type]["x_dim_min"]
        self.x_dim_max = SCENARIOS[scenario_type]["x_dim_max"]
        self.y_dim_min = SCENARIOS[scenario_type]["y_dim_min"]
        self.y_dim_max = SCENARIOS[scenario_type]["y_dim_max"]

        self.goal = torch.tensor(
            [
                (self.x_dim_min + self.x_dim_max) / 2,
                (self.y_dim_min + self.y_dim_max) / 2,
            ],
            device=device,
            dtype=torch.float32,
        ).repeat(
            batch_dim, 1
        )  # Goal that the agent should reach, extended to [batch_size, 2]

        # Visualization
        self.resolution_factor = SCENARIOS[scenario_type][
            "resolution_factor"
        ]  # Default 200

        self.render_origin = [
            (self.x_dim_min + self.x_dim_max) / 2,
            (self.y_dim_min + self.y_dim_max) / 2,
        ]

        self.viewer_size = (
            int(self.world_x_dim * self.resolution_factor),
            int(self.world_y_dim * self.resolution_factor),
        )
        self.viewer_zoom = SCENARIOS[scenario_type]["viewer_zoom"]

        self.max_steering = kwargs.pop(
            "max_steering",
            torch.tensor(AGENTS["max_steering"], device=device, dtype=torch.float32),
        )  # Maximum allowed steering angle in degree
        self.max_speed = kwargs.pop(
            "max_speed", AGENTS["max_speed"]
        )  # Maximum allowed speed in [m/s]
        self.max_acc = kwargs.pop(
            "max_acc", AGENTS["max_acc"]
        )  # Maximum allowed acceleration in [m/s^2]
        self.min_acc = kwargs.pop(
            "min_acc", AGENTS["min_acc"]
        )  # Maximum allowed acceleration in [m/s^2]
        self.max_steering_rate = kwargs.pop(
            "max_steering_rate", AGENTS["max_steering_rate"]
        )  # Maximum allowed steering rate in [radian/s]
        self.min_steering_rate = kwargs.pop(
            "min_steering_rate", AGENTS["min_steering_rate"]
        )  # Maximum allowed steering rate in [radian/s]

        if not hasattr(self, "parameters"):
            self.parameters = Parameters(
                n_agents=kwargs.pop("n_agents", SCENARIOS[scenario_type]["n_agents"]),
                scenario_type=scenario_type,
                is_testing_mode=kwargs.pop("is_testing_mode", False),
                is_visualize_short_term_path=kwargs.pop(
                    "is_visualize_short_term_path", True
                ),
                is_real_time_rendering=kwargs.pop("is_real_time_rendering", False),
                n_points_short_term=kwargs.pop("n_points_short_term", 3),
                dt=kwargs.pop("dt", 0.05),
                is_ego_view=kwargs.pop("is_ego_view", True),
                is_observe_distance_to_center_line=kwargs.pop(
                    "is_observe_distance_to_center_line", True
                ),
                n_steps_stored=kwargs.pop("n_steps_stored", 5),
                is_add_noise=kwargs.pop("is_add_noise", True),
                is_visualize_extra_info=kwargs.pop("is_visualize_extra_info", False),
                render_title=kwargs.pop(
                    "render_title",
                    "Multi-Agent Reinforcement Learning for Road Traffic (CPM Lab Scenario)",
                ),
            )

        self.n_agents = self.parameters.n_agents
        self.colors = colors

        # Logs
        if self.parameters.is_testing_mode:
            print(colored(f"[INFO] Testing mode", "red"))
        print(colored(f"[INFO] Scenario type: {self.parameters.scenario_type}", "red"))

        self.n_agents = self.parameters.n_agents

        self.short_term_ref_path = torch.zeros(
            (batch_dim, self.parameters.n_points_short_term, 2),
            device=device,
            dtype=torch.float32,
        )

        # Timer for the first env
        self.timer = Timer(
            start=time.time(),
            end=0,
            step=torch.zeros(
                batch_dim, device=device, dtype=torch.int32
            ),  # Each environment has its own time step
            step_duration=torch.zeros(
                self.parameters.max_steps, device=device, dtype=torch.float32
            ),
            step_begin=time.time(),
            render_begin=0,
        )

        # The shape of each agent is considered a rectangle with 4 vertices.
        # The first vertex is repeated at the end to close the shape.
        self.vertices = torch.zeros(
            (batch_dim, self.n_agents, 5, 2), device=device, dtype=torch.float32
        )

        weighting_ref_directions = torch.linspace(
            1,
            0.2,
            steps=self.parameters.n_points_short_term,
            device=device,
            dtype=torch.float32,
        )
        weighting_ref_directions /= weighting_ref_directions.sum()
        self.rewards = Rewards(
            progress=torch.tensor(reward_progress, device=device, dtype=torch.float32),
            weighting_ref_directions=weighting_ref_directions,  # Progress in the weighted directions (directions indicating by closer short-term reference points have higher weights)
            higth_v=torch.tensor(reward_vel, device=device, dtype=torch.float32),
            reach_goal=torch.tensor(
                reward_reach_goal, device=device, dtype=torch.float32
            ),
        )
        self.rew = torch.zeros(batch_dim, device=device, dtype=torch.float32)

        self.penalties = Penalties(
            deviate_from_ref_path=torch.tensor(
                penalty_deviate_from_ref_path, device=device, dtype=torch.float32
            ),
            change_steering=torch.tensor(
                penalty_change_steering, device=device, dtype=torch.float32
            ),
            time=torch.tensor(penalty_time, device=device, dtype=torch.float32),
            leave_world=torch.tensor(
                penalty_leave_world, device=device, dtype=torch.float32
            ),
        )

        self.observations = Observations(
            noise_level=torch.tensor(noise_level, device=device, dtype=torch.float32),
        )

        self.observations.past_action_steering = CircularBuffer(
            torch.zeros(
                (self.parameters.n_steps_stored, batch_dim, self.n_agents),
                device=device,
                dtype=torch.float32,
            )
        )

        self.observations.past_action_vel = CircularBuffer(
            torch.zeros(
                (self.parameters.n_steps_stored, batch_dim, self.n_agents),
                device=device,
                dtype=torch.float32,
            )
        )

        self.distances = Distances(
            type="c2c",  # Type of distances between agents
            agents=torch.zeros(
                batch_dim, self.n_agents, self.n_agents, dtype=torch.float32
            ),
            ref_paths=torch.zeros(
                (batch_dim, self.n_agents), device=device, dtype=torch.float32
            ),
        )

        self.thresholds = Thresholds(
            reach_goal=torch.tensor(
                threshold_reach_goal, device=device, dtype=torch.float32
            ),
            deviate_from_ref_path=torch.tensor(
                threshold_deviate_from_ref_path, device=device, dtype=torch.float32
            ),
            change_steering=torch.tensor(
                threshold_change_steering, device=device, dtype=torch.float32
            ).deg2rad(),
            distance_mask_agents=self.agent_length * 5,
        )

        self.constants = Constants(
            env_idx_broadcasting=torch.arange(
                batch_dim, device=device, dtype=torch.int32
            ).unsqueeze(-1),
            empty_action_vel=torch.zeros(
                (batch_dim, self.n_agents), device=device, dtype=torch.float32
            ),
            empty_action_steering=torch.zeros(
                (batch_dim, self.n_agents), device=device, dtype=torch.float32
            ),
        )

        self.normalizers = Normalizers(
            pos=torch.tensor(
                [
                    self.distance_between_points_ref_path
                    * self.parameters.n_points_short_term,
                    self.distance_between_points_ref_path
                    * self.parameters.n_points_short_term,
                ],
                device=device,
                dtype=torch.float32,
            ),
            pos_world=torch.tensor(
                [self.world_x_dim, self.world_y_dim], device=device, dtype=torch.float32
            ),
            v=torch.tensor(self.max_speed, device=device, dtype=torch.float32),
            rot=torch.tensor(2 * torch.pi, device=device, dtype=torch.float32),
            steering=self.max_steering,
            distance_ref=torch.tensor(
                lane_width * 2, device=device, dtype=torch.float32
            ),
        )

        # Store the states of agents at previous several time steps
        self.state_buffer = StateBuffer(
            buffer=torch.zeros(
                (self.parameters.n_steps_stored, batch_dim, self.n_agents, 5),
                device=device,
                dtype=torch.float32,
            ),  # [pos_x, pos_y, rot, vel_x, vel_y],
        )

        self.original_pos = torch.zeros(
            (batch_dim, 2), device=device, dtype=torch.float32
        )

        self.is_leave_world = torch.zeros((batch_dim), device=device, dtype=torch.bool)

    def _init_world(self, batch_dim: int, device: torch.device):
        # Make world
        world = WorldCustom(
            batch_dim,
            device,
            x_semidim=torch.tensor(
                self.world_x_dim, device=device, dtype=torch.float32
            ),
            y_semidim=torch.tensor(
                self.world_y_dim, device=device, dtype=torch.float32
            ),
            dt=self.parameters.dt,
        )
        return world

    def _init_agents(self, world: World):
        # Create agents
        for i in range(self.n_agents):
            agent = Vehicle(
                name=f"agent_{i}",
                shape=Box(length=AGENTS["length"], width=AGENTS["width"]),
                color=tuple(self.colors[i]),
                collide=False,
                render_action=False,
                u_range=[
                    self.max_speed,
                    self.max_steering,
                ],  # Control command serves as velocity command
                u_multiplier=[1, 1],
                max_speed=self.max_speed,
                dynamics=KinematicBicycleModel(  # Use the kinematic bicycle model for each agent
                    l_f=AGENTS["l_f"],
                    l_r=AGENTS["l_r"],
                    max_speed=self.max_speed,
                    max_steering=self.max_steering,
                    max_acc=self.max_acc,
                    min_acc=self.min_acc,
                    max_steering_rate=self.max_steering_rate,
                    min_steering_rate=self.min_steering_rate,
                    device=world.device,
                ),
            )
            world.add_agent(agent)

    def generate_reference_path(self, env_idx=None):
        """
        Generate a fixed number of points along a reference path starting from the agent's current position
        and pointing towards the goal. The points are spaced at a fixed distance apart. The generated path
        is stored in the `self.short_term_ref_path` attribute.

        Parameters:
        env_idx (int, optional): The index of the environment for which to generate the reference path.
                                 If None, generate paths for all environments. Defaults to None.

        The output shape is [batch_size, 1, self.parameters.n_points_short_term, 2].
        """
        env_idx = 0
        # env_idx = None
        env_idx = slice(None) if env_idx is None else env_idx

        pos = self.world.agents[0].state.pos[env_idx].clone()
        original_pos = self.original_pos[env_idx].clone()
        goal = self.goal[env_idx].clone()

        if pos.ndim == 1:
            pos = pos.unsqueeze(0)
        if original_pos.ndim == 1:
            original_pos = original_pos.unsqueeze(0)
        if goal.ndim == 1:
            goal = goal.unsqueeze(0)

        direction = goal - pos

        # Normalize the direction vector
        direction_norm = direction / direction.norm(dim=-1, keepdim=True)

        # Create a range of distances for the points along the path
        distances = (
            torch.arange(
                1, self.parameters.n_points_short_term + 1, device=self.world.device
            ).reshape(-1, 1)
            * self.distance_between_points_ref_path
        )

        # Generate the points along the path
        path_points = pos.unsqueeze(1) + direction_norm.unsqueeze(1) * distances

        # Project reference points to the line pointing from the original position to the goal
        # Calculate the direction vectors from original positions to goals
        direction_orig = goal - original_pos

        # Normalize the direction vectors to unit vectors
        direction_norm_orig = direction_orig / direction_orig.norm(dim=-1, keepdim=True)

        # Calculate vectors from original positions to the short-term reference points
        vectors_to_points = path_points - original_pos.unsqueeze(1)

        # Project the vectors_to_points onto the direction_norm_orig
        proj_length = (
            (vectors_to_points * direction_norm_orig.unsqueeze(1))
            .sum(dim=-1)
            .unsqueeze(-1)
        )
        projected_points = original_pos.unsqueeze(
            1
        ) + proj_length * direction_norm_orig.unsqueeze(1)

        # Assign the projected points to self.short_term_projected
        self.short_term_ref_path[env_idx] = projected_points

    def reset_world_at(self, env_index: int = None, agent_index: int = None):
        """
        This function resets the world at the specified env_index and the specified agent_index.
        If env_index is given as None, the majority part of computation will be done in a vectorized manner.

        Args:
        :param env_index: index of the environment to reset. If None a vectorized reset should be performed
        :param agent_index: index of the agent to reset. If None all agents in the specified environment will be reset.
        """
        env_index_2 = (
            env_index if env_index is not None else slice(None)
        )  # `slice(None)` is equivalent to `:`

        agent = self.world.agents[0]

        self.timer.step_duration[:] = 0
        self.timer.start = time.time()
        self.timer.step_begin = time.time()
        self.timer.end = 0
        self.timer.step[env_index_2] = 0

        if env_index is None:
            batch_size = self.world.batch_dim
            initial_pos = torch.zeros(
                (batch_size, 2), device=self.world.device, dtype=torch.float32
            )
            initial_rot = torch.zeros(
                (batch_size, 1), device=self.world.device, dtype=torch.float32
            )
            initial_steering = torch.zeros(
                (batch_size, 1), device=self.world.device, dtype=torch.float32
            )
            initial_sideslip = torch.zeros(
                (batch_size, 1), device=self.world.device, dtype=torch.float32
            )

            for i in range(batch_size):
                initial_pos[i], initial_rot[i] = self.generate_valid_initial_state(i)

            initial_speed = (
                torch.rand(
                    (batch_size, 1), device=self.world.device, dtype=torch.float32
                )
                * agent.max_speed
            )
        else:
            initial_pos, initial_rot = self.generate_valid_initial_state(env_index)
            initial_speed = (
                torch.rand(1, device=self.world.device, dtype=torch.float32)
                * agent.max_speed
            )
            initial_steering = torch.zeros(
                1, device=self.world.device, dtype=torch.float32
            )
            initial_sideslip = torch.zeros(
                1, device=self.world.device, dtype=torch.float32
            )

        initial_vel = torch.hstack(
            [
                initial_speed * torch.cos(initial_rot + initial_sideslip),
                initial_speed * torch.sin(initial_rot + initial_sideslip),
            ]
        )

        initial_rot = (initial_rot + torch.pi) % (
            2 * torch.pi
        ) - torch.pi  # Convert to [-pi, pi]
        if initial_rot.ndim == 0:
            initial_rot = initial_rot.unsqueeze(0)

        agent.set_pos(initial_pos, batch_index=env_index)
        agent.set_rot(initial_rot, batch_index=env_index)
        agent.set_speed(initial_speed, batch_index=env_index)
        agent.set_vel(initial_vel, batch_index=env_index)
        agent.set_steering(initial_steering, batch_index=env_index)
        agent.set_sideslip_angle(initial_sideslip, batch_index=env_index)

        # Reset the state buffer
        self.state_buffer.reset()
        state_add = torch.cat(
            (
                agent.state.pos,
                agent.state.rot,
                agent.state.vel,
            ),
            dim=-1,
        ).unsqueeze(1)
        self.state_buffer.add(state_add)  # Add new state

        self.original_pos[
            env_index_2
        ] = initial_pos.clone()  # Update priginal positions

        # Distance from the center of gravity (CG) of the agent to its reference path
        self.distances.ref_paths[env_index_2, 0], _ = get_perpendicular_distances(
            point=agent.state.pos[env_index_2],
            polyline=torch.stack(
                [self.original_pos[env_index_2], self.goal[env_index_2]], dim=1
            ),
        )

        # Generate short-term reference path
        self.generate_reference_path(env_index)

        self.is_leave_world[env_index_2] = False

    def generate_valid_initial_state(self, env_idx):
        """
        Generate a valid initial position and rotation for a given environment index.
        """
        valid_position = False
        while not valid_position:
            # Generate random position within the specified range
            pos_x = (
                torch.rand(1).item()
                * (self.x_dim_max - self.x_dim_min - 2 * self.agent_length)
                + self.x_dim_min
                + self.agent_length
            )
            pos_y = (
                torch.rand(1).item()
                * (self.y_dim_max - self.y_dim_min - 2 * self.agent_length)
                + self.y_dim_min
                + self.agent_length
            )
            initial_pos = torch.tensor([pos_x, pos_y], device=self.world.device)

            # Check if the position is valid based on the distance to the goal
            valid_position = (
                initial_pos - self.goal[env_idx]
            ).norm() >= 40 * self.agent_length

        # Calculate the direction to the goal and set the initial rotation
        direction_to_goal = torch.atan2(
            self.goal[env_idx][1] - initial_pos[1],
            self.goal[env_idx][0] - initial_pos[0],
        )
        initial_rot = direction_to_goal + (torch.rand(1).item() - 0.5) * torch.pi * 0.5
        # initial_rot = (torch.rand(1).squeeze(0) - 0.5) * torch.pi * 2  # Random rotation

        return initial_pos, initial_rot

    def reward(self, agent: Agent):
        """
        Issue rewards for the given agent in all envs.
            Positive Rewards:
                Moving forward (become negative if the projection of the moving direction to its reference path is negative)
                Moving forward with high speed (become negative if the projection of the moving direction to its reference path is negative)
                Reaching goal (optional)

            Negative Rewards (penalties):
                Deviating from reference paths
                Changing steering too quick

        Args:
            agent: The agent for which the observation is to be generated.

        Returns:
            A tensor with shape [batch_dim].
        """
        # Initialize
        self.rew[:] = 0

        # Get the index of the current agent
        agent_index = self.world.agents.index(agent)

        # [update] mutual distances between agents, vertices of each agent, and collision matrices
        self._update_state_before_rewarding(agent)

        ##################################################
        ## [reward] forward movement
        ##################################################
        latest_state = self.state_buffer.get_latest(n=1)
        move_vec = (agent.state.pos - latest_state[:, agent_index, 0:2]).unsqueeze(
            1
        )  # Vector of the current movement

        ref_points_vecs = self.short_term_ref_path - latest_state[
            :, agent_index, 0:2
        ].unsqueeze(
            1
        )  # Vectors from the previous position to the points on the short-term reference path
        ref_points_vecs_norm = ref_points_vecs / ref_points_vecs.norm(dim=-1).unsqueeze(
            dim=-1
        )
        move_projected = torch.sum(move_vec * ref_points_vecs_norm, dim=-1)
        move_projected_weighted = torch.matmul(
            move_projected, self.rewards.weighting_ref_directions
        )  # Put more weights on nearing reference points

        reward_movement = (
            move_projected_weighted
            / (agent.max_speed * self.world.dt)
            * self.rewards.progress
        )
        self.rew += reward_movement  # Relative to the maximum possible movement

        ##################################################
        ## [reward] high velocity
        ##################################################
        v_proj = torch.sum(
            agent.state.vel.unsqueeze(1) * ref_points_vecs_norm, dim=-1
        ).mean(-1)
        factor_moving_direction = torch.where(
            v_proj > 0, 1, 2
        )  # Get more penalty if move in negative direction

        reward_vel = (
            factor_moving_direction * v_proj / agent.max_speed * self.rewards.higth_v
        )
        self.rew += reward_vel

        ##################################################
        ## [penalty] deviating from reference path
        ##################################################
        penalty_dev_ref = (
            self.distances.ref_paths[:, agent_index]
            / self.thresholds.deviate_from_ref_path
            * self.penalties.deviate_from_ref_path
        )
        self.rew += penalty_dev_ref

        ##################################################
        ## [penalty] changing steering too quick
        ##################################################
        steering_current = self.observations.past_action_steering.get_latest(n=1)[
            :, agent_index
        ]
        steering_past = self.observations.past_action_steering.get_latest(n=2)[
            :, agent_index
        ]

        steering_change = torch.clamp(
            (steering_current - steering_past).abs() * self.normalizers.steering
            - self.thresholds.change_steering,  # Not forget to denormalize
            min=0,
        )
        steering_change_reward_factor = steering_change / (
            2 * agent.u_range[1] - 2 * self.thresholds.change_steering
        )
        penalty_change_steering = (
            steering_change_reward_factor * self.penalties.change_steering
        )
        self.rew += penalty_change_steering

        ##################################################
        ## [penalty] Leave the world
        ##################################################
        # Check if any agents leave the world
        self.is_leave_world[:] = (
            (agent.state.pos[:, 0] < self.x_dim_min)
            | (agent.state.pos[:, 0] > self.x_dim_max)
            | (agent.state.pos[:, 1] < self.y_dim_min)
            | (agent.state.pos[:, 1] > self.y_dim_max)
        )

        self.rew += self.is_leave_world * self.penalties.leave_world

        ##################################################
        ## [penalty/reward] time
        ##################################################
        # Get time reward if moving in positive direction; otherwise get time penalty
        time_reward = (
            torch.where(v_proj > 0, 1, -1)
            * agent.state.vel.norm(dim=-1)
            / agent.max_speed
            * self.penalties.time
        )
        self.rew += time_reward

        # [update] previous positions and short-term reference paths
        state_add = torch.cat(
            (
                agent.state.pos,
                agent.state.rot,
                agent.state.vel,
            ),
            dim=-1,
        ).unsqueeze(1)
        self.state_buffer.add(state_add)
        self.generate_reference_path()

        assert not self.rew.isnan().any(), "Rewards contain nan."
        assert not self.rew.isinf().any(), "Rewards contain inf."

        # Clamed the reward to avoid abs(reward) being too large
        rew_clamed = torch.clamp(self.rew, min=-1, max=1)

        return rew_clamed

    def _update_state_before_rewarding(self, agent: Vehicle):
        """
        Update some states (such as vertices of the agent) that will be used before rewarding agents.
        """
        # Timer
        self.timer.step_duration[self.timer.step] = time.time() - self.timer.step_begin
        self.timer.step_begin = (
            time.time()
        )  # Set to the current time as the begin of the current time step
        self.timer.step += 1  # Increment step by 1
        # print(self.timer.step)

        self.vertices[:, 0] = get_rectangle_vertices(
            center=agent.state.pos,
            yaw=agent.state.rot,
            width=agent.shape.width,
            length=agent.shape.length,
            is_close_shape=True,
        )

        # Distance from the center of gravity (CG) of the agent to its reference path
        (self.distances.ref_paths[:, 0], _,) = get_perpendicular_distances(
            point=agent.state.pos,
            polyline=torch.stack([self.original_pos, self.goal], dim=1),
        )

    def observation(self, agent: Agent):
        """
        Generate an observation for the given agent in all envs.

        Args:
            agent: The agent for which the observation is to be generated.

        Returns:
            The observation for the given agent in all envs, which consists of the observation of this agent itself and possibly the observation of its surrounding agents.
                The observation of this agent itself includes
                    position (in case of using bird view),
                    rotation (in case of using bird view),
                    velocity,
                    steering, and
                    short-term reference path
        """
        # Define the probability of perturbation
        possibility_perturbation = 0.02

        # Determine which environments should be perturbed
        env_idx = (
            torch.rand(self.world.batch_dim, device=self.world.device)
            < possibility_perturbation
        )

        # Generate random perturbations for x and y directions
        perturbation_x = (
            torch.rand(self.world.batch_dim, device=self.world.device) * 2 - 1
        ) * agent.shape.length
        perturbation_y = (
            torch.rand(self.world.batch_dim, device=self.world.device) * 2 - 1
        ) * agent.shape.length

        # Apply perturbations only to the selected environments
        new_pos = agent.state.pos.clone()
        new_pos[env_idx, 0] += perturbation_x[env_idx]
        new_pos[env_idx, 1] += perturbation_y[env_idx]

        # Inject the new positions back into the agent's state
        agent.set_pos(new_pos, batch_index=None)

        pos = agent.state.pos
        rot = agent.state.rot

        # Compute the vectors from the agent's position to the short-term reference paths
        vectors_to_ref = self.short_term_ref_path - pos.unsqueeze(1)

        # Calculate the angles of these vectors
        angles_to_ref = (
            torch.atan2(vectors_to_ref[..., 1], vectors_to_ref[..., 0])
        ).unsqueeze(-1)

        # Compute the angle difference between the vectors and the agent's rotation
        angle_difference = angles_to_ref - rot.unsqueeze(1)

        # Calculate the length of the vectors
        vector_lengths = torch.norm(vectors_to_ref, dim=-1).unsqueeze(-1)

        # Project the vectors into the agent's ego view using the angle difference
        short_term_ref_ego_view_x = vector_lengths * torch.cos(angle_difference)
        short_term_ref_ego_view_y = vector_lengths * torch.sin(angle_difference)

        short_term_ref_ego_view = torch.cat(
            (short_term_ref_ego_view_x, short_term_ref_ego_view_y), dim=-1
        )

        obs = torch.hstack(
            [
                agent.state.speed / self.normalizers.v,
                agent.state.steering / self.normalizers.steering,
                (short_term_ref_ego_view / self.normalizers.pos).reshape(
                    self.world.batch_dim, -1
                ),
                self.distances.ref_paths / self.normalizers.distance_ref,
            ]
        )

        if self.parameters.is_add_noise:
            # Add sensor noise if required
            obs = obs + (
                self.observations.noise_level
                * torch.rand_like(obs, device=self.world.device, dtype=torch.float32)
            )

        # Add new observation - actions & normalize
        if agent.action.u is None:
            self.observations.past_action_vel.add(self.constants.empty_action_vel)
            self.observations.past_action_steering.add(
                self.constants.empty_action_steering
            )
        else:
            self.observations.past_action_vel.add(
                torch.stack([a.action.u[:, 0] for a in self.world.agents], dim=1)
                / self.normalizers.v
            )
            self.observations.past_action_steering.add(
                torch.stack([a.action.u[:, 1] for a in self.world.agents], dim=1)
                / self.normalizers.steering
            )

        return obs

    def done(self):
        # print("[DEBUG] done()")

        is_max_steps_reached = self.timer.step == (self.parameters.max_steps - 1)

        is_done = is_max_steps_reached

        # Reset agents that reach their respective goal
        distance_to_goal = (self.goal - self.world.agents[0].state.pos).norm(dim=-1)

        is_reach_goal = distance_to_goal <= self.thresholds.reach_goal

        # Reset the env where the agent reaches its goal or leaves the world
        is_reset = is_reach_goal | self.is_leave_world

        # Logs
        # if self.is_leave_world.any():
        #     print("Leave world.")
        # if is_reach_goal.any():
        #     print("Reach goal.")

        for env_idx, reached in enumerate(is_reset):
            if reached:
                self.reset_world_at(env_idx)

        return is_done

    def info(self, agent: Agent) -> Dict[str, Tensor]:
        """
        This function computes the info dict for "agent" in a vectorized way
        The returned dict should have a key for each info of interest and the corresponding value should
        be a tensor of shape (n_envs, info_size)

        Implementors can access the world at "self.world"

        To increase performance, tensors created should have the device set, like:
        torch.tensor(..., device=self.world.device)

        :param agent: Agent batch to compute info of
        :return: info: A dict with a key for each info of interest, and a tensor value  of shape (n_envs, info_size)
        """
        agent_index = self.world.agents.index(agent)  # Index of the current agent

        is_action_empty = agent.action.u is None

        # Observation of the policy in the CBF module, which includes:
        # self-observation: speed
        # self-observation: distance to left lane boundary
        # self-observation: distance to right lane boundary
        # others-observation: vertices of surrounding, observable agents
        # others-observation: velocities of surrounding, observable agents
        # others-observation: jaw angles of surrounding, observable agents
        # others-observation: short-term reference path of surrounding, observable agents
        cbf_obs = None
        # self.observations.

        info = {
            "pos": agent.state.pos / self.normalizers.pos_world,
            "rot": angle_eliminate_two_pi(agent.state.rot) / self.normalizers.rot,
            "vel": agent.state.vel / self.normalizers.v,
            "act_vel": (
                (agent.action.u[:, 0] / self.normalizers.v)
                if not is_action_empty
                else self.constants.empty_action_vel[:, agent_index]
            ),
            "act_steer": (
                (agent.action.u[:, 1] / self.normalizers.steering)
                if not is_action_empty
                else self.constants.empty_action_steering[:, agent_index]
            ),
            "ref": (self.short_term_ref_path[:] / self.normalizers.pos_world).reshape(
                self.world.batch_dim, -1
            ),
            "distance_ref": self.distances.ref_paths[:, agent_index]
            / self.normalizers.distance_ref,
        }

        return info

    def extra_render(self, env_index: int = 0):
        if self.parameters.is_real_time_rendering:
            if self.timer.step[0] == 0:
                pause_duration = 0  # Not sure how long should the simulation be paused at time step 0, so rather 0
            else:
                pause_duration = self.world.dt - (time.time() - self.timer.render_begin)
            if pause_duration > 0:
                time.sleep(pause_duration)
            # print(f"Paused for {pause_duration} sec.")

            self.timer.render_begin = time.time()  # Update

        geoms = []

        if self.parameters.is_visualize_short_term_path:
            geom = rendering.PolyLine(
                v=self.short_term_ref_path[env_index],
                close=False,
            )
            xform = rendering.Transform()
            geom.add_attr(xform)
            geom.set_color(*self.colors[0])
            geoms.append(geom)

            for i_p in self.short_term_ref_path[env_index]:
                circle = rendering.make_circle(radius=0.01, filled=True)
                xform = rendering.Transform()
                circle.add_attr(xform)
                xform.set_translation(i_p[0], i_p[1])
                circle.set_color(*self.colors[0])
                geoms.append(circle)

        # Render world boundaries as a rectangle
        bottom_left = (self.x_dim_min, self.y_dim_min)
        bottom_right = (self.x_dim_max, self.y_dim_min)
        top_right = (self.x_dim_max, self.y_dim_max)
        top_left = (self.x_dim_min, self.y_dim_max)

        # Create lines for each side of the rectangle
        boundary_lines = [
            rendering.Line(bottom_left, bottom_right, width=2),
            rendering.Line(bottom_right, top_right, width=2),
            rendering.Line(top_right, top_left, width=2),
            rendering.Line(top_left, bottom_left, width=2),
        ]

        # Set color and add each line to the geoms
        for line in boundary_lines:
            xform = rendering.Transform()
            line.add_attr(xform)
            line.set_color(*Color.black100)
            geoms.append(line)

        # Visualize goal
        color = self.colors[4]
        circle = rendering.make_circle(radius=self.thresholds.reach_goal, filled=True)
        xform = rendering.Transform()
        circle.add_attr(xform)
        xform.set_translation(self.goal[env_index, 0], self.goal[env_index, 1])
        circle.set_color(*color)
        geoms.append(circle)

        self._render_extra_info(geoms)

        return geoms

    def _render_extra_info(self, geoms):
        if self.parameters.is_visualize_extra_info:
            hight_a = -0.10
            hight_b = -0.20
            hight_c = -0.30

            # Title
            geom = rendering.TextLine(
                text=self.parameters.render_title,
                x=0.05 * self.resolution_factor,
                y=(self.world.y_semidim + hight_a) * self.resolution_factor,
                font_size=14,
            )
            xform = rendering.Transform()
            geom.add_attr(xform)
            geoms.append(geom)

            # Time and time step
            geom = rendering.TextLine(
                text=f"t: {self.timer.step[0]*self.parameters.dt:.2f} sec",
                x=0.05 * self.resolution_factor,
                y=(self.world.y_semidim + hight_b) * self.resolution_factor,
                font_size=14,
            )
            xform = rendering.Transform()
            geom.add_attr(xform)
            geoms.append(geom)

            geom = rendering.TextLine(
                text=f"n: {self.timer.step[0]}",
                x=0.05 * self.resolution_factor,
                y=(self.world.y_semidim + hight_c) * self.resolution_factor,
                font_size=14,
            )
            xform = rendering.Transform()
            geom.add_attr(xform)
            geoms.append(geom)


if __name__ == "__main__":
    scenario = GoalReaching()
    render_interactively(
        scenario=scenario,
        control_two_agents=False,
        shared_reward=False,
    )
